[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "iHuman Lab Research Manual",
    "section": "",
    "text": "Welcome\nWe are glad that you have decided to join Intelligent Human-Machine Nexus Lab (iHuman Lab)! We are really excited to have you as a part of our team and we will do what we can to make sure that you have a great time in our lab. We hope you will learn a lot about human and machines, and neuroscience, develop new and useful skills (coding, data analysis, writing papers, giving talks), make new friends, and have a great deal of fun throughout the whole process!!\nThis lab manual is crafted as your essential companion, designed to acquaint you with our lab’s standard workflow. Within these pages, you will find detailed explanations writing and coding practices, learning resources for Python, BCI, and physiological data processing. These workflows ensure smooth integration into our research.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgment",
    "href": "index.html#acknowledgment",
    "title": "iHuman Lab Research Manual",
    "section": "Acknowledgment",
    "text": "Acknowledgment\nThis lab manual draws inspiration from various sources, and in some sections, content has been adapted or reproduced from these sources. We acknowledge the following works for their influence:\nContext Dynamics Lab, The Memory Modulation Lab, Aly Lab, Peelle Lab, Ritchey Lab, Kemp Lab\nThis lab manual is licensed under the Creative Commons Attribution 4.0 International License. You are free to share, adapt, and redistribute the material in any medium or format, provided appropriate credit is given to the original authors and the source is properly cited.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Who is this lab manual for?\nEvery new lab member should read the latest version of this lab manual in detail and reference it later as needed. Periodically throughout the document, you will see callouts with listed Task items.\nCompleting your read-through entails:\nThis lab manual is meant to be a living document. All lab members are welcome (and encouraged!) to submit edits that improve the content, clarity, and overall helpfulness of this document at any point throughout their tenure in the lab. Knowledge is ever changing, you might come across new tools, methods, and resources which might help you and future lab members. You can share that knowledge through this manual",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-is-this-lab-manual-for",
    "href": "introduction.html#who-is-this-lab-manual-for",
    "title": "Introduction",
    "section": "",
    "text": "Reading the contents of the manual\nAsking current lab members about any confusing aspects\nCompleting the relevant Task items. You will also see non-task NOTE items; these provide helpful tips and additional commentary on the nearby text.\n\n\n\n\n\n\n\n\nTask\n\n\n\nUpon reading through this lab manual for the first time, please update the document to include your name in the Lab members section. Importantly, be sure to fork the GitHub repository, make your edit on your personal fork, and submit a pull request with your update.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#why-is-it-worth-my-time-to-read-through-the-manual",
    "href": "introduction.html#why-is-it-worth-my-time-to-read-through-the-manual",
    "title": "Introduction",
    "section": "Why is it worth my time to read through the manual?",
    "text": "Why is it worth my time to read through the manual?\nAside from pursuing your own curiosity, automating aspects of your research can significantly reduce the mundane tasks that consume valuable time and effort. By implementing automated processes, you free up mental energy and resources that can then be redirected towards exploring more complex and thought-provoking questions within your field.\nThis manual serves as a foundational guide to help you establish an efficient workflow for automating your research tasks. By following these steps, you will streamline data collection, analysis, and synthesis processes, allowing you to focus more on formulating innovative hypotheses, designing insightful experiments, and interpreting results in novel ways.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#what-isnt-this-lab-manual",
    "href": "introduction.html#what-isnt-this-lab-manual",
    "title": "Introduction",
    "section": "What isn’t this lab manual?",
    "text": "What isn’t this lab manual?\nThis lab manual should not be regarded as a rigid set of rules to be strictly adhered to. Rather, it serves as a collection of established workflows that have been utilized by others and found beneficial. It acknowledges that individual preferences and circumstances vary, and therefore, not every aspect of these workflows may align perfectly with your own methods or preferences.\nThe workflows outlined here are meant to be flexible tools that can be adjusted and refined according to the unique requirements of your research projects.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#research-project-manual",
    "href": "introduction.html#research-project-manual",
    "title": "Introduction",
    "section": "Research project manual",
    "text": "Research project manual\nAlong with this general lab manual, we will also use the research project manual which provides comprehensive guidance on coding practices, the procedure for writing research papers, managing data effectively, structuring research projects, and accessing valuable learning resources. This supplementary resource is essential for ensuring that our research endeavors are conducted with precision, thoroughness, and adherence to best practices across all stages of our project development and documentation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html",
    "href": "coding_practices/general_practices.html",
    "title": "Coding practices",
    "section": "",
    "text": "Effective code management is vital in research, especially for data analysis and simulations. We try to follow a standard workflow for developing Python code detailed below:\n\nEstablishing a Structured Development Environment\n\nUtilize Conda: Begin by configuring Miniconda or Anaconda to effectively manage isolated development environments.\nEnvironment Configuration: Tailor environments for each project with Conda, ensuring precise control over dependencies.\nEnforce Consistency: Adopt tools like Ruff to maintain code formatting standards, promoting clarity and maintainability.\n\nOrganizing and Structuring Your Codebase\n\nProject Organization: Implement a systematic folder structure (data/, notebooks/, src/, tests/, scripts/) to streamline project navigation and maintain order.\nVersion Control: Leverage Git for comprehensive version tracking and GitHub for collaborative development management.\nDocumentation: Enhance transparency and usability with thorough code comments and a detailed README.md to articulate project objectives and usage guidelines.\n\nEffective Data Management Practices\n\nData Organization: Store raw data in data/raw/ and processed data in data/processed/ to uphold data integrity and accessibility.\nVersioning: Employ robust data management tools to manage versions, ensuring reproducibility and traceability.\n\nEnsuring Reproducibility and Continuity\n\nEnvironment Replication: Share standardized environment configurations to facilitate seamless project setup across different systems.\nDocumentation Updates: Maintain updated documentation and Jupyter Notebooks to ensure reproducibility and facilitate knowledge transfer.\n\nFacilitating Collaborative Excellence\n\nCode Review: Foster a culture of quality assurance through regular code reviews, utilizing pull requests to incorporate feedback and improvements.\nCommunication: Promote effective communication channels to enhance collaboration, share insights, and address challenges collectively.\n\nSustaining Long-term Project Integrity\n\nContinuous Improvement: Stay current with tool updates and library advancements to optimize project performance and reliability.\nIssue Management: Utilize issue tracking tools to promptly address bugs and enhancements, ensuring project stability and growth.\n\n\nBy adhering to these principles, you establish a framework that not only enhances the efficiency and reliability of your coding practices but also fosters a professional environment conducive to innovation and collaborative success. Whether leveraging Conda for environment management, adhering to coding standards with Ruff, or maintaining meticulous documentation, each step contributes to a disciplined and effective approach to coding excellence in research.",
    "crumbs": [
      "Coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#maintain-code-cleanliness",
    "href": "coding_practices/general_practices.html#maintain-code-cleanliness",
    "title": "Good coding practices",
    "section": "",
    "text": "Meaningful Naming\n\nUse Descriptive Names\nNames should clearly convey the purpose of the variables, functions, and classes. Avoid generic names like data or temp unless their context is obvious.\nExample:\n# Bad naming\ndef func(x):\n    return x * 2\n\n# Good naming\ndef double_value(number):\n    return number * 2\n\n\nUse Consistent Naming Conventions\nFollow consistent naming conventions to improve readability. In Python, the conventions are:\n\nVariables and functions: Use snake_case (e.g., total_amount, calculate_sum).\nClasses: Use CamelCase (e.g., InvoiceManager, DataProcessor).\nConstants: Use UPPER_CASE (e.g., MAX_RETRIES, DEFAULT_TIMEOUT).\n\n\n\n\nSmall and Focused Functions\n\nSingle Responsibility Principle\nEach function should do one thing and do it well. Break down complex functions into smaller, more manageable ones.\nExample:\n# Bad: Single function with multiple responsibilities\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n# Good: Functions with a single responsibility\ndef validate_order(order):\n    # validation logic\n    pass\n\ndef calculate_total(order):\n    # total calculation logic\n    pass\n\ndef save_order_to_database(order):\n    # save logic\n    pass\n\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n\nAvoid Deep Nesting\nDeeply nested code can be hard to follow. Use early returns to reduce nesting levels.\nExample:\n# Bad: Deep nesting\ndef process_data(data):\n    if data is not None:\n        if len(data) &gt; 0:\n            if isinstance(data, list):\n                # process the list\n                pass\n\n# Good: Reduced nesting\ndef process_data(data):\n    if data is None or len(data) == 0 or not isinstance(data, list):\n        return\n\n    # process the list\n    pass\n\n\n\nClear and Consistent Formatting\n\nFollow PEP 8\nAdhere to PEP 8, the Python style guide, to ensure consistent formatting. Key guidelines include:\n\nIndentation: Use 4 spaces per indentation level.\nLine Length: Limit all lines to a maximum of 79 characters.\nBlank Lines: Use blank lines to separate functions, classes, and sections within functions.\n\nTo make things easy, we will use ruff as a standard in our lab.\nExample:\n# PEP 8 compliant code\ndef calculate_area(radius):\n    import math\n    return math.pi * (radius ** 2)\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return calculate_area(self.radius)\n\n\nUse Docstrings\nUse docstrings to document your modules, classes, and functions. Docstrings should describe the purpose, parameters, and return values.\nExample:\ndef calculate_area(radius):\n    \"\"\"\n    Calculate the area of a circle given its radius.\n\n    Args:\n        radius (float): The radius of the circle.\n\n    Returns:\n        float: The area of the circle.\n    \"\"\"\n    import math\n    return math.pi * (radius ** 2)\n\n\n\nError Handling\n\nUse Exceptions Appropriately\nHandle exceptions gracefully and use specific exception types to avoid catching unexpected errors.\nExample:\n# Bad: Catching all exceptions\ntry:\n    result = 10 / 0\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Good: Catching specific exceptions\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError as e:\n    print(f\"Cannot divide by zero: {e}\")\n\n\nAvoid Empty except Clauses\nEmpty except clauses can hide errors and make debugging difficult. Always handle exceptions with proper logging or user feedback.\nExample:\n# Bad: Empty except clause\ntry:\n    # risky operation\n    pass\nexcept:\n    pass\n\n# Good: Proper exception handling\ntry:\n    # risky operation\n    pass\nexcept ValueError as e:\n    print(f\"ValueError occurred: {e}\")\n\n\n\nRefactoring\nRegularly review and refactor code to improve structure and readability. Refactoring can include renaming variables, breaking down functions, or reorganizing code.\nKeeping your code tidy ensures it is readable and maintainable. Clean code is easier to understand, debug, and enhance. Here are some tips for maintaining code cleanliness:\nFor more detailed style guide, we will use Google Python Style Guide.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#write-decoupled-code",
    "href": "coding_practices/general_practices.html#write-decoupled-code",
    "title": "Good coding practices",
    "section": "Write Decoupled Code",
    "text": "Write Decoupled Code\nIn a research lab, Python code often forms the backbone of data analysis, simulation, and experimentation. However, as research projects grow in complexity and involve multiple researchers, writing well-structured and decoupled code becomes crucial. Decoupled code—code that is modular and has minimal interdependencies—enhances readability, maintainability, and testability, ultimately leading to more robust and scalable research tools. This chapter explores the principles of writing decoupled Python code and offers practical guidance tailored to the needs of a research lab environment.\n\nUnderstanding Decoupling in Code\nDecoupling refers to the practice of reducing dependencies between different parts of a program. When code is decoupled, changes in one part of the code have minimal impact on others. This is particularly important in research labs where:\n\nMultiple Researchers: Different team members might work on different aspects of the codebase.\nEvolving Requirements: Research projects often evolve, requiring frequent changes and updates to the code.\nIntegration of Tools: Code might need to interface with various tools, libraries, and datasets.\n\nThe primary benefits of decoupling are:\n\nModularity: Breaking down the code into distinct modules or components.\nTestability: Isolated components are easier to test independently.\nMaintainability: Changes in one module have less chance of breaking other parts of the code.\nReusability: Modular components can be reused across different projects or experiments.\n\n\n\nPrinciples of Decoupled Python Code\n\nSingle Responsibility Principle (SRP)\nEach module or class should have only one reason to change, meaning it should have one primary responsibility. For instance, a module handling data cleaning should not be involved in data visualization.\nExample: Suppose you have a module named data_processing.py. It should focus solely on data cleaning and preprocessing, while another module, data_visualization.py, should handle plotting and generating graphs.\n# data_processing.py\ndef clean_data(data):\n    # Implementation of data cleaning\n    pass\n\ndef preprocess_data(data):\n    # Implementation of data preprocessing\n    pass\n\n# data_visualization.py\nimport matplotlib.pyplot as plt\n\ndef plot_data(data):\n    plt.plot(data)\n    plt.show()\n\n\nDependency Injection\nAvoid hard-coding dependencies within your functions or classes. Instead, pass dependencies as arguments. This approach makes your code more flexible and easier to test.\nExample: Instead of hardcoding a data source, pass it as a parameter.\n# Instead of this\ndef analyze_data():\n    data = load_data_from_file('data.csv')\n    # Process data\n\n# Use dependency injection\ndef analyze_data(data_loader):\n    data = data_loader()\n    # Process data\n\n# Example data loader function\ndef load_data_from_file():\n    return read_csv('data.csv')\n\n\nUse Interfaces and Abstract Classes\nDefine abstract classes or interfaces to specify the methods that concrete implementations should provide. This approach helps in decoupling the code from specific implementations and allows easier replacement or modification of components.\nExample:\nfrom abc import ABC, abstractmethod\n\nclass DataLoader(ABC):\n    @abstractmethod\n    def load(self):\n        pass\n\nclass CSVDataLoader(DataLoader):\n    def load(self):\n        # Load data from CSV\n        pass\n\nclass JSONDataLoader(DataLoader):\n    def load(self):\n        # Load data from JSON\n        pass\n\n\nSeparation of Concerns\nEnsure that different aspects of your code are managed separately. For example, keep data handling, computation, and presentation concerns distinct from each other.\nExample:\n# data_manager.py\ndef load_data(filename):\n    # Code to load data\n    pass\n\n# analysis_engine.py\ndef perform_analysis(data):\n    # Code to analyze data\n    pass\n\n# report_generator.py\ndef generate_report(results):\n    # Code to generate report\n    pass\n\n\n\nConclusion\nDecoupled Python code is essential for maintaining clarity, flexibility, and reliability in a research lab setting. By adhering to principles such as the Single Responsibility Principle, dependency injection, and separation of concerns, researchers can create codebases that are easier to understand, test, and modify. Implementing these practices ensures that research software remains adaptable and robust, supporting the dynamic needs of scientific inquiry.\nFor more information on writing decoupled code, explore here.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#test-your-code",
    "href": "coding_practices/general_practices.html#test-your-code",
    "title": "Good coding practices",
    "section": "Test Your Code",
    "text": "Test Your Code\nIn the research lab environment, testing Python code is crucial for ensuring accuracy, reliability, and reproducibility of scientific results. pytest is one of the most popular testing frameworks in Python due to its simplicity, flexibility, and powerful features.\n\nBasic Test Structure\npytest automatically discovers and runs tests based on their naming conventions. Tests should be placed in files named test_*.py or *_test.py, and test functions should start with test_.\nExample:\n# test_calculator.py\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(-1, 1) == 0\nRun tests using the pytest command:\npytest\n\n\nWriting Tests with pytest\n\nBasic Assertions\nUse assert statements to check if the code behaves as expected. pytest will report failed assertions with detailed information.\nExample:\ndef multiply(x, y):\n    return x * y\n\ndef test_multiply():\n    assert multiply(2, 3) == 6\n    assert multiply(0, 5) == 0\n\n\nUsing Fixtures\nFixtures are used to set up and tear down resources needed for tests. They are defined using the @pytest.fixture decorator and can be scoped to functions, classes, modules, or sessions.\nExample:\nimport pytest\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\ndef test_sum(sample_data):\n    assert sum(sample_data) == 15\n\n\nParameterized Tests\nParameterized tests allow you to run the same test function with multiple sets of inputs using the @pytest.mark.parametrize decorator.\nExample:\nimport pytest\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 2, 3),\n    (-1, 1, 0),\n    (2, 2, 4),\n])\ndef test_add(a, b, expected):\n    assert add(a, b) == expected\n\n\nTesting for Exceptions\nYou can check for expected exceptions using the pytest.raises context manager.\nExample:\nimport pytest\n\ndef divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n\ndef test_divide():\n    with pytest.raises(ValueError):\n        divide(1, 0)\n\n\nCustomizing Test Output\npytest provides options for customizing test output, such as verbosity levels and formatting.\nExample:\nRun tests with verbose output:\npytest -v\nGenerate a test report in a JUnit-compatible format:\npytest --junitxml=report.xml\n\n\n\nAdvanced Features of pytest\n\nPlugins\npytest supports a wide range of plugins to enhance its functionality. Some popular plugins include:\n\npytest-cov: Provides code coverage reporting.\npytest-mock: Simplifies mocking of objects and functions.\npytest-xdist: Allows parallel test execution and distributed testing.\n\nInstall and use plugins via pip:\npip install pytest-cov pytest-mock\nExample with pytest-cov:\npytest --cov=my_module\n\n\nFixtures with Scope and Autouse\nControl the scope and automatic application of fixtures using the scope and autouse parameters.\nExample:\nimport pytest\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef setup_module():\n    print(\"\\nSetting up module...\")\n    yield\n    print(\"\\nTearing down module...\")\n\n\n\nIntegrating pytest into the Research Workflow\n\nContinuous Integration (CI)\nIntegrate pytest with CI/CD pipelines to automatically run tests on code changes. Tools like GitHub Actions, GitLab CI, and Jenkins support pytest integration.\nExample with GitHub Actions:\nCreate a .github/workflows/test.yml file:\nname: Run Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n      - name: Install dependencies\n        run: |\n          pip install pytest\n      - name: Run tests\n        run: |\n          pytest\n\n\nCode Coverage\nUse pytest-cov to measure test coverage and ensure that critical parts of your code are tested.\nExample:\npytest --cov=my_module --cov-report=html\n\n\nTest Documentation\nDocument your tests to explain their purpose and expected outcomes, which helps new team members understand the testing strategy and rationale.\n\n\n\nConclusion\npytest is a powerful and flexible testing framework that enhances the reliability and maintainability of Python code in a research lab setting. By leveraging pytest’s features such as fixtures, parameterized tests, and plugins, researchers can create comprehensive test suites that ensure code quality and facilitate collaboration. Integrating pytest into your research workflow, including continuous integration and code coverage, will help maintain the integrity of your codebase and support reproducible research.\nBy adopting these practices, you can build a robust testing framework that contributes to more reliable and effective scientific research.\nLearn more about testing here.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#document-your-code",
    "href": "coding_practices/general_practices.html#document-your-code",
    "title": "Good coding practices",
    "section": "",
    "text": "Write docstrings for all public modules, functions, classes, and methods: Follow the conventions of PEP 257.\nUse comments to explain the why, not the what: Comments should provide context and rationale.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#document-your-project",
    "href": "coding_practices/general_practices.html#document-your-project",
    "title": "Good coding practices",
    "section": "Document Your Project",
    "text": "Document Your Project\nProject-level documentation helps users understand the purpose of the project, how to install it, and how to get started.\n\nCreate a README file: This should include an overview of the project, installation instructions, usage examples, and links to further documentation.\nUse a consistent structure: Organize documentation into sections like Introduction, Installation, Usage, Contributing, and License.\nLeverage tools like Sphinx for generating documentation: Sphinx can convert your docstrings and markdown files into beautiful, searchable HTML documentation.\n\nCheck out Sphinx for generating project documentation.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#foster-collaboration",
    "href": "coding_practices/general_practices.html#foster-collaboration",
    "title": "Good coding practices",
    "section": "Foster Collaboration",
    "text": "Foster Collaboration\nMaking your project social encourages collaboration and contributions from the community.\n\nHost your code on a platform like GitHub or GitLab: These platforms provide version control and facilitate collaboration.\nEncourage contributions: Include a CONTRIBUTING.md file with guidelines for contributing to the project.\nBe responsive to issues and pull requests: Engage with contributors by providing feedback and merging changes promptly.\n\nLearn how to foster collaboration on GitHub and GitLab.\nBy following these practices, you can write clean, maintainable, and well-documented Python code that is easy to test and collaborate on.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/project_structure.html",
    "href": "coding_practices/project_structure.html",
    "title": "Project structure",
    "section": "",
    "text": "At iHuman lab, we follow the project structure (shown below) serves as a flexible guide rather than a strict mandate. It is designed to provide a clear framework for organizing our research endeavors effectively. The structure encompasses essential components such as documentation in the docs/ directory, configuration files in config/, and a dedicated ‘data/’ directory for managing datasets. The notebooks/ directory houses Jupyter notebooks for exploratory analysis, while the src/ directory contains modularized source code for data processing, feature engineering, model development, visualization, and utility functions. Unit tests are stored in tests/ to ensure the reliability of our implementations. While this structure offers a solid foundation, it remains adaptable to meet the specific needs and nuances of each research project we undertake, allowing for customization and refinement as required.\nresearch_project_name/\n│\n├── README.md                       # Project overview and instructions\n├── docs/                           # Documentation\n│\n├── config/                         # Configuration files\n│   ├── config.yaml                 # Configuration file for parameters\n│   └── logging.yaml                # Configuration file for logging\n│\n├── data/                           # Directory for datasets\n│   ├── raw/                        # Raw data files (immutable)\n│   ├── processed/                  # Processed data files (generated)\n│   └── interim/                    # Intermediate data files (temporary)\n│\n├── notebooks/                      # Jupyter notebooks for exploration\n│   └── exploratory_analysis.ipynb\n│\n├── src/                            # Source code for the project\n│   ├── __init__.py\n│   ├── main.py                     # Main script to run the project\n│   ├── data/                       # Module for data processing\n│   │   ├── __init__.py\n│   │   ├── preprocess.py           # Preprocessing functions\n│   │   └── load_data.py            # Data loading functions\n│   ├── dataset/                    # Module for creating datasets\n│   │   ├── __init__.py\n│   │   └── create_dataset.py       # Functions to create datasets\n│   ├── features/                   # Module for creating features\n│   │   ├── __init__.py\n│   │   └── feature_engineering.py  # Feature engineering functions\n│   ├── models/                     # Module for defining models\n│   │   ├── __init__.py\n│   │   └── model.py                # Model architecture and training functions\n│   ├── visualization/              # Visualization module\n│   │   ├── __init__.py\n│   │   └── visualize.py            # Visualization functions\n│   └── utils.py                    # Utility functions used across modules\n│\n├── tests/                          # Unit tests\n│\n└── environment.yml                 # Conda env file specifying dependencies\nTo facilitate the easy creation and adoption of this project structure across our research lab, we have opted to use iHuman Lab’s Cookiecutter template.\nTo use the template\n$ pip install cookiecutter\nor\n$ conda config --add channels conda-forge\n$ conda install cookiecutter\nTo start a new project, run:\n$ cookiecutter https://github.com/iHuman-Lab/ihuman-cookiecutter-data-science.git",
    "crumbs": [
      "Coding practices",
      "Project structure"
    ]
  },
  {
    "objectID": "python_resources.html",
    "href": "python_resources.html",
    "title": "Python resources",
    "section": "",
    "text": "General resources",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#websites",
    "href": "python_resources.html#websites",
    "title": "Python resources",
    "section": "Websites",
    "text": "Websites\nAre you gearing up for job interviews or seeking internship opportunities in the realm of programming? Mastering Python can significantly enhance your skillset and boost your competitiveness in the job market. Here’s a tailored selection of platforms designed to equip you with essential Python skills and prepare you effectively for technical assessments:\n\nEdx If you are new to python programming, definitely take Introduction to Computer Science and Programming Using Python course. It is one of the best courses out there from which you can learn lot of fundamental python programming skills.\nHackerRank HackerRank has lot of resources to learn many programming languages. Regestration is free.\nLeetCode LeetCode is much intensive platform to learn coding. It can be used if you want to prepare for interviews.",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#videos",
    "href": "python_resources.html#videos",
    "title": "Python resources",
    "section": "Videos",
    "text": "Videos\nPyCon is one of the best way to get in touch with great Python community. Lot of great talks and tutorials (highly recommended!).",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#pytorch",
    "href": "python_resources.html#pytorch",
    "title": "Python resources",
    "section": "Pytorch",
    "text": "Pytorch\nAs a new student, Pytroch will be an essential tool for your deep learning research due to its flexibility and extensive community support. To streamline your workflow and boost productivity, we’ve compiled a guide to key PyTorch libraries and tools that simplify model development, training, and evaluation.\nA github repository with lot of resources: The incredible Pytorch\n\nPyTorch Lightning\nGitHub Link: PyTorch Lightning\nPyTorch Lightning is our preferred framework for developing deep learning models in the lab. It automates the tedious aspects of deep learning research, such as setting up training loops, logging metrics, and handling distributed training across multiple GPUs or TPUs. By standardizing best practices and providing a clean interface, PyTorch Lightning enables rapid prototyping and seamless scaling of research projects.\nKey Features:\n\nAutomated Training Loop: Write minimal code to define your model, optimizer, and data loaders, and PyTorch Lightning takes care of the rest.\nReproducibility: Ensures deterministic training behavior for reproducible research results.\nScalability: Easily scale your models from single GPU to multi-GPU or even distributed training without changing your codebase.\nExperiment Management: Built-in support for logging, visualization, and tracking experiments, making it easier to monitor and compare model performance.\n\nIn our lab, we predominantly use PyTorch Lightning for its ability to accelerate the development cycle and ensure consistency across experiments. It encourages best practices in deep learning research and facilitates collaboration among team members.\n\n\nOther Pytorch Related Links\n\nhttps://github.com/szymonmaszke/torchfunc\nhttps://github.com/wkentaro/pytorch-for-numpy-users\nhttps://github.com/suriyadeepan/torchtest",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#links",
    "href": "python_resources.html#links",
    "title": "Python resources",
    "section": "Links",
    "text": "Links\n\nData Science Related\n\nhttps://github.com/rasbt/mlxtend\nhttps://github.com/slundberg/shap\nhttps://github.com/ericmjl/pyjanitor\nhttps://esa.github.io/pagmo2/index.html\nhttps://github.com/modin-project/modin\nhttps://docs.metaflow.org\nhttps://perceptilabs.readme.io/docs/welcome\n\n\n\nMisc\n\nhttps://github.com/ray-project/ray (A very good parallel processing library)",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "bci_resources.html",
    "href": "bci_resources.html",
    "title": "BCI resources",
    "section": "",
    "text": "These are some of the resource to introduce you to basics of brain computer interfaces (BCI). The resources contain videos, articles, books. Please feel free to come to iHuman lab and ask anyone at lab to get more information.\n\nVideos\n\nSwartz Center for Computational Neuroscience\n\nIntroduction to BCI and EEG\n\nStart with the initial videos (video-1 to video-7) to grasp the fundamentals of BCI and EEG.\nThese videos provide a solid foundation for understanding how brain activity can be used to control external devices.\nWatch the videos here\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThese introductory videos are highly recommended as they cover the basics you need to get started.\n\n\n\n\nMike X Cohen\n\nGeneral Neural Time-Series Analysis\n\nExplore advanced concepts in neural time-series analysis, focusing on methods to analyze and interpret brain signals over time.\nThe videos primarily use Matlab, but you can replicate the analysis in Python using the mne package.\nWatch the videos here\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdapting Matlab scripts to Python will not only deepen your understanding of neural data analysis but also enhance your proficiency in Python programming.\n\n\n\n\n\nGitHub Links\n\nNeuroTechX/awesome-bci\n\nThis repository is a curated collection of resources, tools, and projects related to BCI research.\nExplore various software tools, datasets, and community-driven projects that can aid your research.\nVisit the repository\n\n\n\n\nBooks\n\nBrain-Computer Interfacing: An Introduction by Rajesh P.N. Rao\n\nThis book provides a comprehensive introduction to BCI technology, covering its principles, applications, and future trends.\nAccess the book\n\nAnalyzing Neural Time Series Data: Theory and Practice by Mike X. Cohen\n\nLearn detailed methodologies and practical techniques for analyzing neural time-series data.\nGain insights into advanced signal processing techniques applicable to BCI research.\nExplore the book\n\n\n\n\nMNE Python\nAnother important package, we use extensively in our lab processing the physiological data is MNE-Python. MNE-Python is a Python package specifically designed for the analysis of brain signals, collected using techniques like magnetoencephalography (MEG) and electroencephalography (EEG). It provides a robust set of tools for processing, analyzing, and visualizing neural data, making it a preferred choice for researchers and clinicians in the field of neuroscience.\nKey features of MNE-Python include:\n\nData Handling: Import and manipulate data from various formats such as .fif, .edf, and .bdf.\nPreprocessing: Remove noise, filter data, and interpolate bad channels.\nAnalysis: Perform time-domain and frequency-domain analyses, including time-frequency decomposition.\nVisualization: Plot data in 2D and 3D to visualize sensor and source-space information.\nSource Localization: Estimate the location of neural sources responsible for recorded signals.\nStatistics: Conduct statistical tests to analyze differences in brain activity.\n\nMNE-Python supports a wide range of functionalities tailored to both novice users and advanced researchers, facilitating comprehensive exploration and interpretation of neuroimaging data.\nFor more information and detailed documentation, visit MNE-Python website.\n\nOther links\n\nTutorial on MNE python\nOfficial MNE tutorials\n\n\n\n\n\n\n\nTask\n\n\n\nGo through the tutorials and implement standard functions using the EEG data. Reach out to Hemanth to get the EEG data.\n\n\n\n\n\nLab Streaming Layer (LSL)\nLab streaming layer (LSL) is an open-source networked middleware ecosystem to stream, receive, synchronize, and record neural, physiological, and behavioral data streams acquired from diverse sensor hardware.\nIt reduces complexity and barriers to entry for researchers, sensor manufacturers, and users through a simple, interoperable, standardized API to connect data consumers to data producers while abstracting obstacles such as platform differences, stream discovery, synchronization and fault-tolerance.\nWe use LSL as a standard platform to interface multiple sensor and simulation softwares in our lab.\n\nOverview and Importance\n\nLSL is a crucial middleware platform used in our lab to synchronize and record data from various sensors.\nIt simplifies data integration across different hardware and software platforms, ensuring seamless connectivity and interoperability.\nLearn more about LSL",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "bci_resources.html#videos",
    "href": "bci_resources.html#videos",
    "title": "BCI resources",
    "section": "",
    "text": "Great set of videos which introduce the basics of BCI and EEG (highly recommended!) by Swartz Center for Computational Neuroscience.\n\n\n\n\n\n\n\nTip\n\n\n\nFirst few videos are enough to get started (video-1 to video-7). Once you get started with EEG analysis you can watch the rest.\n\n\n\nIf you want to get into general neural time-series analysis here is a great set of videos by Mike X Cohen.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nEven though the emphasis is on use of Matlab, you can do all the analysis using a python package called mne.\nIt is also a great opportunity to port the Matlab codes used in the lectures to Python through which you can learn Python also!",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "bci_resources.html#online",
    "href": "bci_resources.html#online",
    "title": "BCI resources",
    "section": "Online",
    "text": "Online\nThis repository contains some awesome resources on BCI: NeuroTechX/awesome-bci",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "bci_resources.html#books",
    "href": "bci_resources.html#books",
    "title": "BCI resources",
    "section": "Books",
    "text": "Books\nAmazing introduction to BCI\n\nBrain-computer interfacing : an introduction / Rajesh P.N. Rao.\n\nTo analyse neural time series data (a great book!)\n\nAnalyzing neural time series data : theory and practice / Mike X. Cohen.",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "bci_resources.html#github-links",
    "href": "bci_resources.html#github-links",
    "title": "BCI resources",
    "section": "GitHub links",
    "text": "GitHub links\nThis repository contains some awesome resources on BCI: NeuroTechX/awesome-bci",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "eeg_resources.html",
    "href": "eeg_resources.html",
    "title": "EEG resources",
    "section": "",
    "text": "EEG Glossary\nTo understand EEG terminology and report formats, refer to the comprehensive glossary by clinical electroencephalographers: EEG Glossary.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#setting-up-eeg-recordings",
    "href": "eeg_resources.html#setting-up-eeg-recordings",
    "title": "EEG resources",
    "section": "Setting up EEG Recordings",
    "text": "Setting up EEG Recordings\nWatch this video tutorial on setting up advanced brain monitoring electrodes for EEG recording:\n\nFeel free to drop by the lab for more information.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#eeg-analysis-steps",
    "href": "eeg_resources.html#eeg-analysis-steps",
    "title": "EEG resources",
    "section": "EEG Analysis Steps",
    "text": "EEG Analysis Steps\nFor those who are starting with EEG analysis, it can be overwhelming to go across a lot of steps. Here is almost a standard procedure for a lot of EEG analysis. Each block is a research area by itself. There are a lot of papers on each block. Do not get scared once you start the analysis of some EEG data, these steps will become second to your nature. Do read below given paper about how others are doing these steps. We use mne-python to implement different block functionalities. If you want more information, refer to Mike X Cohen’s lectures\n\n\n\neeg-analysis",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#mne",
    "href": "eeg_resources.html#mne",
    "title": "EEG resources",
    "section": "MNE",
    "text": "MNE\nIn the lab, we extensively use mne-python for analysis of a lot of EEG (time series in general) data. It is an amazing package with a lot of development and an amazing community. If you get a chance, do join the mailing list.\nHere are some resources to get you started with mne-python\n\nBasic mne-python\nIntermediate mne-python tutorials",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#eeg-databases",
    "href": "eeg_resources.html#eeg-databases",
    "title": "EEG resources",
    "section": "EEG Databases",
    "text": "EEG Databases\nHere are some databases on which you are apply your mne-python skills.\n\n\n\n\n\n\nTip\n\n\n\nThe best way to learn a (Python) package is to use it on your own data and see what are the capabilities\n\n\nPick up some data from this repository: meagmohit/EEG-Datasets. Use mne-python do some analysis and that is how you get introduced to analysis of EEG.\n\n\n\n\n\n\nTip\n\n\n\nIf you have doubts, do drop by to iHuman lab and ask any questions!",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#papers",
    "href": "eeg_resources.html#papers",
    "title": "EEG resources",
    "section": "Papers",
    "text": "Papers\n\nBest Practices in Data Analysis and Sharing in Neuroimaging using MEEG.\nA Reproducible MEG/EEG Group Study With the MNE Software: Recommendations, Quality Assessments, and Good Practices\nEEG artifact removal—state-of-the-art and guidelines",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#github-links",
    "href": "eeg_resources.html#github-links",
    "title": "EEG resources",
    "section": "GitHub Links",
    "text": "GitHub Links\nExplore these repositories and resources for EEG and signal processing:\n\nAnalysis Related Repositories\n\npyprep\nautoreject\neegfaster\nneurodsp\nBioSPPy\nEMG-Signal-Processing-Library\nPython-for-Signal-Processing\n\n\n\nUseful Libraries and Links\n\nBest Python Libraries for Psychology\nNeuroDataDesign\nEEG Notebooks\nOpenTools\nSWIPE4ICA for EEG artifact identification\nLabeling Tool\n\n\n\nDeep Learning with Physiological Signals\n\nARL EEG Models\nDeepEEG\nBraindecode\nMNE-Torch\n\n\nThis guide aims to provide you with foundational resources and tools to excel in EEG research. Don’t hesitate to visit the iHuman lab for further assistance and hands-on training. Happy researching!",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "checklist_and_signature.html",
    "href": "checklist_and_signature.html",
    "title": "Checklist and signature page",
    "section": "",
    "text": "Congrats on making towards the end of the document. Please sign the below document and send it to ihuman.research.lab@gmail.com\n\nLoading…",
    "crumbs": [
      "Checklist and signature page"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#small-and-focused-functions",
    "href": "coding_practices/general_practices.html#small-and-focused-functions",
    "title": "Good coding practices",
    "section": "2. Small and Focused Functions",
    "text": "2. Small and Focused Functions\n\n2.1. Single Responsibility Principle\nEach function should do one thing and do it well. Break down complex functions into smaller, more manageable ones.\nExample:\n# Bad: Single function with multiple responsibilities\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n# Good: Functions with a single responsibility\ndef validate_order(order):\n    # validation logic\n    pass\n\ndef calculate_total(order):\n    # total calculation logic\n    pass\n\ndef save_order_to_database(order):\n    # save logic\n    pass\n\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n\n2.2. Avoid Deep Nesting\nDeeply nested code can be hard to follow. Use early returns to reduce nesting levels.\nExample:\n# Bad: Deep nesting\ndef process_data(data):\n    if data is not None:\n        if len(data) &gt; 0:\n            if isinstance(data, list):\n                # process the list\n                pass\n\n# Good: Reduced nesting\ndef process_data(data):\n    if data is None or len(data) == 0 or not isinstance(data, list):\n        return\n\n    # process the list\n    pass",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#clear-and-consistent-formatting",
    "href": "coding_practices/general_practices.html#clear-and-consistent-formatting",
    "title": "Good coding practices",
    "section": "3. Clear and Consistent Formatting",
    "text": "3. Clear and Consistent Formatting\n\n3.1. Follow PEP 8\nAdhere to PEP 8, the Python style guide, to ensure consistent formatting. Key guidelines include:\n\nIndentation: Use 4 spaces per indentation level.\nLine Length: Limit all lines to a maximum of 79 characters.\nBlank Lines: Use blank lines to separate functions, classes, and sections within functions.\n\nExample:\n# PEP 8 compliant code\ndef calculate_area(radius):\n    import math\n    return math.pi * (radius ** 2)\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return calculate_area(self.radius)\n\n\n3.2. Use Docstrings\nUse docstrings to document your modules, classes, and functions. Docstrings should describe the purpose, parameters, and return values.\nExample:\ndef calculate_area(radius):\n    \"\"\"\n    Calculate the area of a circle given its radius.\n\n    Args:\n        radius (float): The radius of the circle.\n\n    Returns:\n        float: The area of the circle.\n    \"\"\"\n    import math\n    return math.pi * (radius ** 2)",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#error-handling",
    "href": "coding_practices/general_practices.html#error-handling",
    "title": "Good coding practices",
    "section": "4. Error Handling",
    "text": "4. Error Handling\n\n4.1. Use Exceptions Appropriately\nHandle exceptions gracefully and use specific exception types to avoid catching unexpected errors.\nExample:\n# Bad: Catching all exceptions\ntry:\n    result = 10 / 0\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Good: Catching specific exceptions\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError as e:\n    print(f\"Cannot divide by zero: {e}\")\n\n\n4.2. Avoid Empty except Clauses\nEmpty except clauses can hide errors and make debugging difficult. Always handle exceptions with proper logging or user feedback.\nExample:\n# Bad: Empty except clause\ntry:\n    # risky operation\n    pass\nexcept:\n    pass\n\n# Good: Proper exception handling\ntry:\n    # risky operation\n    pass\nexcept ValueError as e:\n    print(f\"ValueError occurred: {e}\")",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#refactoring",
    "href": "coding_practices/general_practices.html#refactoring",
    "title": "Good coding practices",
    "section": "5. Refactoring",
    "text": "5. Refactoring\n\n5.1. Regular Refactoring\nRegularly review and refactor code to improve structure and readability. Refactoring can include renaming variables, breaking down functions, or reorganizing code.\n\n\n5.2. Use Refactoring Tools\nUtilize tools and IDE features that help identify code smells or areas for improvement, such as flake8 for style violations or black for automatic formatting.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#testing",
    "href": "coding_practices/general_practices.html#testing",
    "title": "Good coding practices",
    "section": "6. Testing",
    "text": "6. Testing\n\n6.1. Write Unit Tests\nUnit tests ensure that individual components of your code work as expected. Use frameworks like unittest or pytest to write and run tests.\nExample:\nimport unittest\n\ndef add(a, b):\n    return a + b\n\nclass TestMathOperations(unittest.TestCase):\n    def test_add(self):\n        self.assertEqual(add(2, 3), 5)\n        self.assertEqual(add(-1, 1), 0)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n6.2. Test Coverage\nAim for high test coverage to ensure your code is well-tested. Use tools like coverage.py to measure how much of your code is covered by tests.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#use-version-control",
    "href": "coding_practices/general_practices.html#use-version-control",
    "title": "Good coding practices",
    "section": "7. Use Version Control",
    "text": "7. Use Version Control\n\n7.1. Commit Frequently with Meaningful Messages\nUse version control systems like Git to track changes and collaborate with others. Commit changes frequently and use descriptive commit messages.\nExample:\n# Good commit message\ngit commit -m \"Refactor data processing logic to improve readability\"\n\n# Bad commit message\ngit commit -m \"Update code\"\n\n\n7.2. Branching and Merging\nUse branches to manage different features or fixes and merge them into the main branch when they are ready. This practice helps in keeping the main branch stable and functional.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#conclusion",
    "href": "coding_practices/general_practices.html#conclusion",
    "title": "Good coding practices",
    "section": "Conclusion",
    "text": "Conclusion\nBy following these best practices, you can ensure that your Python code remains clean, readable, and maintainable. Adopting these practices not only improves the quality of your code but also enhances collaboration and productivity within development teams. Remember that clean code is an ongoing commitment, and regularly reviewing and improving your codebase will contribute to its long-term success and adaptability.\nKeeping your code tidy ensures it is readable and maintainable. Clean code is easier to understand, debug, and enhance. Here are some tips for maintaining code cleanliness:\n\nUse meaningful variable names: Choose names that clearly describe the purpose of the variable.\nFollow PEP 8 guidelines: PEP 8 is the style guide for Python code, recommending best practices for formatting and structuring your code.\nKeep functions short and focused: A function should do one thing and do it well.\nRemove unnecessary comments and code: Comments should explain why something is done, not what is done.\n\nFor more details, refer to the PEP 8 style guide and the Google Python Style Guide.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#chapter-6-writing-tests-for-python-code-in-a-research-lab-with-pytest",
    "href": "coding_practices/general_practices.html#chapter-6-writing-tests-for-python-code-in-a-research-lab-with-pytest",
    "title": "Good coding practices",
    "section": "Chapter 6: Writing Tests for Python Code in a Research Lab with pytest",
    "text": "Chapter 6: Writing Tests for Python Code in a Research Lab with pytest\n\nIntroduction\nIn the research lab environment, testing Python code is crucial for ensuring accuracy, reliability, and reproducibility of scientific results. pytest is one of the most popular testing frameworks in Python due to its simplicity, flexibility, and powerful features. This chapter provides an in-depth discussion of how to use pytest effectively for testing Python code in a research lab setting.\n\n\n6.1 Why Use pytest?\npytest is widely favored because of its:\n\nUser-Friendly Syntax: Writing tests in pytest is straightforward and requires less boilerplate code compared to other frameworks.\nRich Plugin Ecosystem: pytest has a rich ecosystem of plugins that extend its functionality, such as for test reporting, coverage, and mocking.\nPowerful Fixtures: Fixtures in pytest provide a flexible way to manage setup and teardown code for tests.\nDetailed Reporting: pytest offers detailed test failure reports, making it easier to diagnose issues.\nParameterized Tests: pytest supports parameterized tests, allowing you to run the same test with different inputs efficiently.\n\n\n\n6.2 Setting Up pytest\nTo start using pytest, follow these steps:\n\n6.2.1 Installation\nInstall pytest using pip:\npip install pytest\n\n\n6.2.2 Basic Test Structure\npytest automatically discovers and runs tests based on their naming conventions. Tests should be placed in files named test_*.py or *_test.py, and test functions should start with test_.\nExample:\n# test_calculator.py\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(-1, 1) == 0\nRun tests using the pytest command:\npytest\n\n\n\n6.3 Writing Tests with pytest\n\n6.3.1 Basic Assertions\nUse assert statements to check if the code behaves as expected. pytest will report failed assertions with detailed information.\nExample:\ndef multiply(x, y):\n    return x * y\n\ndef test_multiply():\n    assert multiply(2, 3) == 6\n    assert multiply(0, 5) == 0\n\n\n6.3.2 Using Fixtures\nFixtures are used to set up and tear down resources needed for tests. They are defined using the @pytest.fixture decorator and can be scoped to functions, classes, modules, or sessions.\nExample:\nimport pytest\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\ndef test_sum(sample_data):\n    assert sum(sample_data) == 15\n\n\n6.3.3 Parameterized Tests\nParameterized tests allow you to run the same test function with multiple sets of inputs using the @pytest.mark.parametrize decorator.\nExample:\nimport pytest\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 2, 3),\n    (-1, 1, 0),\n    (2, 2, 4),\n])\ndef test_add(a, b, expected):\n    assert add(a, b) == expected\n\n\n6.3.4 Testing for Exceptions\nYou can check for expected exceptions using the pytest.raises context manager.\nExample:\nimport pytest\n\ndef divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n\ndef test_divide():\n    with pytest.raises(ValueError):\n        divide(1, 0)\n\n\n6.3.5 Customizing Test Output\npytest provides options for customizing test output, such as verbosity levels and formatting.\nExample:\nRun tests with verbose output:\npytest -v\nGenerate a test report in a JUnit-compatible format:\npytest --junitxml=report.xml\n\n\n\n6.4 Advanced Features of pytest\n\n6.4.1 Plugins\npytest supports a wide range of plugins to enhance its functionality. Some popular plugins include:\n\npytest-cov: Provides code coverage reporting.\npytest-mock: Simplifies mocking of objects and functions.\npytest-xdist: Allows parallel test execution and distributed testing.\n\nInstall and use plugins via pip:\npip install pytest-cov pytest-mock\nExample with pytest-cov:\npytest --cov=my_module\n\n\n6.4.2 Fixtures with Scope and Autouse\nControl the scope and automatic application of fixtures using the scope and autouse parameters.\nExample:\nimport pytest\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef setup_module():\n    print(\"\\nSetting up module...\")\n    yield\n    print(\"\\nTearing down module...\")\n\n\n\n6.5 Integrating pytest into the Research Workflow\n\n6.5.1 Continuous Integration (CI)\nIntegrate pytest with CI/CD pipelines to automatically run tests on code changes. Tools like GitHub Actions, GitLab CI, and Jenkins support pytest integration.\nExample with GitHub Actions:\nCreate a .github/workflows/test.yml file:\nname: Run Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n      - name: Install dependencies\n        run: |\n          pip install pytest\n      - name: Run tests\n        run: |\n          pytest\n\n\n6.5.2 Code Coverage\nUse pytest-cov to measure test coverage and ensure that critical parts of your code are tested.\nExample:\npytest --cov=my_module --cov-report=html\n\n\n6.5.3 Test Documentation\nDocument your tests to explain their purpose and expected outcomes, which helps new team members understand the testing strategy and rationale.\n\n\n\nConclusion\npytest is a powerful and flexible testing framework that enhances the reliability and maintainability of Python code in a research lab setting. By leveraging pytest’s features such as fixtures, parameterized tests, and plugins, researchers can create comprehensive test suites that ensure code quality and facilitate collaboration. Integrating pytest into your research workflow, including continuous integration and code coverage, will help maintain the integrity of your codebase and support reproducible research.\nBy adopting these practices, you can build a robust testing framework that contributes to more reliable and effective scientific research.\nLearn more about testing here.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html",
    "href": "coding_practices/clean_code.html",
    "title": "Code cleanliness",
    "section": "",
    "text": "Meaningful Naming",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/code_decoupling.html",
    "href": "coding_practices/code_decoupling.html",
    "title": "Decoupled code",
    "section": "",
    "text": "Understanding Decoupling in Code\nDecoupling refers to the practice of reducing dependencies between different parts of a program. When code is decoupled, changes in one part of the code have minimal impact on others. This is particularly important in research labs where:\nThe primary benefits of decoupling are:",
    "crumbs": [
      "Coding practices",
      "Decoupled code"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html",
    "href": "coding_practices/code_testing.html",
    "title": "Code testing",
    "section": "",
    "text": "Basic Test Structure\npytest automatically discovers and runs tests based on their naming conventions. Tests should be placed in files named test_*.py or *_test.py, and test functions should start with test_.\nExample:\nRun tests using the pytest command:",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html",
    "href": "coding_practices/code_documentation.html",
    "title": "Code documentation",
    "section": "",
    "text": "Importance of Documentation\nDocumentation serves several crucial purposes:",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#usage",
    "href": "coding_practices/code_documentation.html#usage",
    "title": "Code documentation",
    "section": "Usage",
    "text": "Usage\nTo analyze data, use the following commands:\nfrom data_analysis_toolkit import DataAnalyzer\n\ndata = [1, 2, 3, 4, 5]\nanalyzer = DataAnalyzer(data)\nmean = analyzer.calculate_mean()\nprint(mean)\n\nTools for Generating Documentation\n\nSphinx\nSphinx is a documentation generator that creates HTML, PDF, and other formats from reStructuredText files. It is often used for creating comprehensive project documentation.\n\nInstallation:\n\npip install sphinx\n\nUsage:\n\nCreate a Sphinx documentation directory:\nsphinx-quickstart\nGenerate documentation:\nsphinx-build -b html source build\n\n\n\nBest Practices for Documentation\n\nWrite Clear and Concise Descriptions\nEnsure that descriptions are clear and avoid jargon. Provide enough detail to understand the purpose and functionality of the code without being overly verbose.\n\n\nUpdate Documentation Regularly\nKeep documentation up-to-date with code changes. Outdated documentation can lead to confusion and errors.\n\n\nUse Consistent Style\nFollow consistent formatting and style guidelines throughout the documentation. This includes using the same terminology, formatting conventions, and level of detail.\n\n\nInclude Examples\nProvide examples to demonstrate how to use functions, classes, or modules. Examples help users understand practical applications of the code.\n\n\nDocument Edge Cases and Limitations\nHighlight any edge cases, limitations, or known issues in the documentation. This prepares users for potential problems and guides them in handling exceptions.\n\n\n\nConclusion\nDocumenting Python code in a research lab is crucial for maintaining clarity, facilitating collaboration, and ensuring reproducibility. By utilizing code comments, docstrings, README files, and inline documentation, researchers can create comprehensive and useful documentation. Tools like Sphinx, MkDocs, and pdoc can automate and enhance the documentation process.\nAdhering to best practices for writing clear and concise documentation will support effective communication and collaboration within the research team and help ensure that your code is usable and maintainable over time. By integrating these practices, you contribute to a well-organized and reliable research environment, fostering better understanding and reproducibility of scientific work.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#contributing",
    "href": "coding_practices/code_documentation.html#contributing",
    "title": "Code documentation",
    "section": "Contributing",
    "text": "Contributing\nContributions are welcome! Please submit a pull request or issue for any improvements.\n\n#### 7.2.4 Inline Documentation\n\nFor larger projects, inline documentation can include design decisions, algorithms, and overall architecture. This helps others understand complex parts of the codebase.\n\n**Example**:\n\n```python\ndef sort_data(data):\n    \"\"\"\n    Sort a list of data using the merge sort algorithm.\n\n    Merge sort is a divide-and-conquer algorithm that divides the data into smaller chunks,\n    recursively sorts them, and then merges the sorted chunks.\n\n    Args:\n        data (list): The data to be sorted.\n\n    Returns:\n        list: The sorted data.\n    \"\"\"\n    if len(data) &lt;= 1:\n        return data\n    middle = len(data) // 2\n    left_half = sort_data(data[:middle])\n    right_half = sort_data(data[middle:])\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    \"\"\"\n    Merge two sorted lists into a single sorted list.\n\n    Args:\n        left (list): The first sorted list.\n        right (list): The second sorted list.\n\n    Returns:\n        list: The merged sorted list.\n    \"\"\"\n    merged = []\n    left_index, right_index = 0, 0\n    while left_index &lt; len(left) and right_index &lt; len(right):\n        if left[left_index] &lt; right[right_index]:\n            merged.append(left[left_index])\n            left_index += 1\n        else:\n            merged.append(right[right_index])\n            right_index += 1\n    merged.extend(left[left_index:])\n    merged.extend(right[right_index:])\n    return merged\n\n7.3 Tools for Generating Documentation\n\n7.3.1 Sphinx\nSphinx is a documentation generator that creates HTML, PDF, and other formats from reStructuredText files. It is often used for creating comprehensive project documentation.\n\nInstallation:\n\npip install sphinx\n\nUsage:\n\nCreate a Sphinx documentation directory:\nsphinx-quickstart\nGenerate documentation:\nsphinx-build -b html source build\n\n\n7.3.2 MkDocs\nMkDocs is a static site generator designed for project documentation. It uses Markdown for writing documentation and provides a range of themes and plugins.\n\nInstallation:\n\npip install mkdocs\n\nUsage:\n\nCreate a new MkDocs project:\nmkdocs new my-project\ncd my-project\nmkdocs serve\nGenerate documentation:\nmkdocs build\n\n\n7.3.3 pdoc\npdoc is a simple documentation generator for Python projects that creates HTML documentation from docstrings.\n\nInstallation:\n\npip install pdoc\n\nUsage:\n\nGenerate documentation:\npdoc --html my_module\n\n\n\n7.4 Best Practices for Documentation\n\n7.4.1 Write Clear and Concise Descriptions\nEnsure that descriptions are clear and avoid jargon. Provide enough detail to understand the purpose and functionality of the code without being overly verbose.\n\n\n7.4.2 Update Documentation Regularly\nKeep documentation up-to-date with code changes. Outdated documentation can lead to confusion and errors.\n\n\n7.4.3 Use Consistent Style\nFollow consistent formatting and style guidelines throughout the documentation. This includes using the same terminology, formatting conventions, and level of detail.\n\n\n7.4.4 Include Examples\nProvide examples to demonstrate how to use functions, classes, or modules. Examples help users understand practical applications of the code.\n\n\n7.4.5 Document Edge Cases and Limitations\nHighlight any edge cases, limitations, or known issues in the documentation. This prepares users for potential problems and guides them in handling exceptions.\n\n\n\nConclusion\nDocumenting Python code in a research lab is crucial for maintaining clarity, facilitating collaboration, and ensuring reproducibility. By utilizing code comments, docstrings, README files, and inline documentation, researchers can create comprehensive and useful documentation. Tools like Sphinx, MkDocs, and pdoc can automate and enhance the documentation process.\nAdhering to best practices for writing clear and concise documentation will support effective communication and collaboration within the research team and help ensure that your code is usable and maintainable over time. By integrating these practices, you contribute to a well-organized and reliable research environment, fostering better understanding and reproducibility of scientific work.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#importance-of-documentation",
    "href": "coding_practices/code_documentation.html#importance-of-documentation",
    "title": "Code documentation",
    "section": "",
    "text": "Clarity: Helps researchers understand the code’s purpose and functionality.\nMaintenance: Facilitates easier updates and debugging by providing insights into the code’s structure and logic.\nCollaboration: Ensures that team members can work effectively on shared codebases.\nReproducibility: Documents the steps and methods used in analysis, enabling others to replicate experiments.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#types-of-documentation",
    "href": "coding_practices/code_documentation.html#types-of-documentation",
    "title": "Code documentation",
    "section": "Types of Documentation",
    "text": "Types of Documentation\nEffective documentation covers various aspects of a codebase:\n\nCode Comments\nComments within the code explain specific lines or sections of code, making it easier to understand the logic and intentions of the code.\n\nInline Comments: Provide explanations for individual lines of code.\nBlock Comments: Explain larger sections or functions.\n\nExample:\ndef calculate_statistics(data):\n    \"\"\"\n    Calculate and return basic statistics for the given data.\n\n    Args:\n        data (list): A list of numerical values.\n\n    Returns:\n        tuple: A tuple containing the mean and standard deviation of the data.\n    \"\"\"\n    mean = sum(data) / len(data)  # Calculate mean\n    variance = sum((x - mean) ** 2 for x in data) / len(data)  # Calculate variance\n    std_dev = variance ** 0.5  # Calculate standard deviation\n    return mean, std_dev\n\n\nDocstrings\nDocstrings are used to describe the purpose, parameters, and return values of functions, classes, and modules. They are essential for generating documentation automatically and integrating with IDEs and documentation tools.\n\nFunction Docstrings: Explain the purpose, arguments, and return values of a function.\nClass Docstrings: Describe the class’s purpose, attributes, and methods.\nModule Docstrings: Provide an overview of the module and its contents.\n\nExample:\nclass DataAnalyzer:\n    \"\"\"\n    A class for analyzing and processing data.\n\n    Attributes:\n        data (list): The data to be analyzed.\n\n    Methods:\n        calculate_mean(): Returns the mean of the data.\n        calculate_median(): Returns the median of the data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the DataAnalyzer with data.\n\n        Args:\n            data (list): The data to be analyzed.\n        \"\"\"\n        self.data = data\n\n    def calculate_mean(self):\n        \"\"\"\n        Calculate and return the mean of the data.\n\n        Returns:\n            float: The mean of the data.\n        \"\"\"\n        return sum(self.data) / len(self.data)\n\n\nREADME Files\nREADME files provide an overview of the project, including its purpose, installation instructions, usage guidelines, and contribution details. They are often the first point of contact for users and contributors.\nExample:\n# Data Analysis Toolkit\n\n# Overview\nThis toolkit provides various utilities for analyzing and processing data.\n\n# Installation\nTo install the toolkit, run:\n```bash\npip install data-analysis-toolkit",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#tools-for-generating-documentation",
    "href": "coding_practices/code_documentation.html#tools-for-generating-documentation",
    "title": "Code documentation",
    "section": "Tools for Generating Documentation",
    "text": "Tools for Generating Documentation\n\nSphinx\nSphinx is a documentation generator that creates HTML, PDF, and other formats from reStructuredText files. It is often used for creating comprehensive project documentation.\n\nInstallation:\n\npip install sphinx\n\nUsage:\n\nCreate a Sphinx documentation directory:\nsphinx-quickstart\nGenerate documentation:\nsphinx-build -b html source build",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#best-practices-for-documentation",
    "href": "coding_practices/code_documentation.html#best-practices-for-documentation",
    "title": "Code documentation",
    "section": "Best Practices for Documentation",
    "text": "Best Practices for Documentation\n\nWrite Clear and Concise Descriptions\nEnsure that descriptions are clear and avoid jargon. Provide enough detail to understand the purpose and functionality of the code without being overly verbose.\n\n\nUpdate Documentation Regularly\nKeep documentation up-to-date with code changes. Outdated documentation can lead to confusion and errors.\n\n\nUse Consistent Style\nFollow consistent formatting and style guidelines throughout the documentation. This includes using the same terminology, formatting conventions, and level of detail.\n\n\nInclude Examples\nProvide examples to demonstrate how to use functions, classes, or modules. Examples help users understand practical applications of the code.\n\n\nDocument Edge Cases and Limitations\nHighlight any edge cases, limitations, or known issues in the documentation. This prepares users for potential problems and guides them in handling exceptions.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#conclusion",
    "href": "coding_practices/code_documentation.html#conclusion",
    "title": "Code documentation",
    "section": "Conclusion",
    "text": "Conclusion\nDocumenting Python code in a research lab is crucial for maintaining clarity, facilitating collaboration, and ensuring reproducibility. By utilizing code comments, docstrings, README files, and inline documentation, researchers can create comprehensive and useful documentation. Tools like Sphinx, MkDocs, and pdoc can automate and enhance the documentation process.\nAdhering to best practices for writing clear and concise documentation will support effective communication and collaboration within the research team and help ensure that your code is usable and maintainable over time. By integrating these practices, you contribute to a well-organized and reliable research environment, fostering better understanding and reproducibility of scientific work.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html#basic-test-structure",
    "href": "coding_practices/code_testing.html#basic-test-structure",
    "title": "Code testing",
    "section": "",
    "text": "# test_calculator.py\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(-1, 1) == 0\n\npytest",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html#writing-tests-with-pytest",
    "href": "coding_practices/code_testing.html#writing-tests-with-pytest",
    "title": "Code testing",
    "section": "Writing Tests with pytest",
    "text": "Writing Tests with pytest\n\nBasic Assertions\nUse assert statements to check if the code behaves as expected. pytest will report failed assertions with detailed information.\nExample:\ndef multiply(x, y):\n    return x * y\n\ndef test_multiply():\n    assert multiply(2, 3) == 6\n    assert multiply(0, 5) == 0\n\n\nUsing Fixtures\nFixtures are used to set up and tear down resources needed for tests. They are defined using the @pytest.fixture decorator and can be scoped to functions, classes, modules, or sessions.\nExample:\nimport pytest\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\ndef test_sum(sample_data):\n    assert sum(sample_data) == 15\n\n\nParameterized Tests\nParameterized tests allow you to run the same test function with multiple sets of inputs using the @pytest.mark.parametrize decorator.\nExample:\nimport pytest\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 2, 3),\n    (-1, 1, 0),\n    (2, 2, 4),\n])\ndef test_add(a, b, expected):\n    assert add(a, b) == expected\n\n\nTesting for Exceptions\nYou can check for expected exceptions using the pytest.raises context manager.\nExample:\nimport pytest\n\ndef divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n\ndef test_divide():\n    with pytest.raises(ValueError):\n        divide(1, 0)\n\n\nCustomizing Test Output\npytest provides options for customizing test output, such as verbosity levels and formatting.\nExample:\nRun tests with verbose output:\npytest -v\nGenerate a test report in a JUnit-compatible format:\npytest --junitxml=report.xml",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html#advanced-features-of-pytest",
    "href": "coding_practices/code_testing.html#advanced-features-of-pytest",
    "title": "Code testing",
    "section": "Advanced Features of pytest",
    "text": "Advanced Features of pytest\n\nPlugins\npytest supports a wide range of plugins to enhance its functionality. Some popular plugins include:\n\npytest-cov: Provides code coverage reporting.\npytest-mock: Simplifies mocking of objects and functions.\npytest-xdist: Allows parallel test execution and distributed testing.\n\nInstall and use plugins via pip:\npip install pytest-cov pytest-mock\nExample with pytest-cov:\npytest --cov=my_module\n\n\nFixtures with Scope and Autouse\nControl the scope and automatic application of fixtures using the scope and autouse parameters.\nExample:\nimport pytest\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef setup_module():\n    print(\"\\nSetting up module...\")\n    yield\n    print(\"\\nTearing down module...\")",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html#integrating-pytest-into-the-research-workflow",
    "href": "coding_practices/code_testing.html#integrating-pytest-into-the-research-workflow",
    "title": "Code testing",
    "section": "Integrating pytest into the Research Workflow",
    "text": "Integrating pytest into the Research Workflow\n\nContinuous Integration (CI)\nIntegrate pytest with CI/CD pipelines to automatically run tests on code changes. Tools like GitHub Actions, GitLab CI, and Jenkins support pytest integration.\nExample with GitHub Actions:\nCreate a .github/workflows/test.yml file:\nname: Run Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n      - name: Install dependencies\n        run: |\n          pip install pytest\n      - name: Run tests\n        run: |\n          pytest\n\n\nCode Coverage\nUse pytest-cov to measure test coverage and ensure that critical parts of your code are tested.\nExample:\npytest --cov=my_module --cov-report=html\n\n\nTest Documentation\nDocument your tests to explain their purpose and expected outcomes, which helps new team members understand the testing strategy and rationale.",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/code_testing.html#conclusion",
    "href": "coding_practices/code_testing.html#conclusion",
    "title": "Code testing",
    "section": "Conclusion",
    "text": "Conclusion\npytest is a powerful and flexible testing framework that enhances the reliability and maintainability of Python code in a research lab setting. By leveraging pytest’s features such as fixtures, parameterized tests, and plugins, researchers can create comprehensive test suites that ensure code quality and facilitate collaboration. Integrating pytest into your research workflow, including continuous integration and code coverage, will help maintain the integrity of your codebase and support reproducible research.\nBy adopting these practices, you can build a robust testing framework that contributes to more reliable and effective scientific research.\nLearn more about testing here.",
    "crumbs": [
      "Coding practices",
      "Code testing"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html#meaningful-naming",
    "href": "coding_practices/clean_code.html#meaningful-naming",
    "title": "Code cleanliness",
    "section": "",
    "text": "Use Descriptive Names\nNames should clearly convey the purpose of the variables, functions, and classes. Avoid generic names like data or temp unless their context is obvious.\nExample:\n# Bad naming\ndef func(x):\n    return x * 2\n\n# Good naming\ndef double_value(number):\n    return number * 2\n\n\nUse Consistent Naming Conventions\nFollow consistent naming conventions to improve readability. In Python, the conventions are:\n\nVariables and functions: Use snake_case (e.g., total_amount, calculate_sum).\nClasses: Use CamelCase (e.g., InvoiceManager, DataProcessor).\nConstants: Use UPPER_CASE (e.g., MAX_RETRIES, DEFAULT_TIMEOUT).",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html#small-and-focused-functions",
    "href": "coding_practices/clean_code.html#small-and-focused-functions",
    "title": "Code cleanliness",
    "section": "Small and Focused Functions",
    "text": "Small and Focused Functions\n\nSingle Responsibility Principle\nEach function should do one thing and do it well. Break down complex functions into smaller, more manageable ones.\nExample:\n# Bad: Single function with multiple responsibilities\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n# Good: Functions with a single responsibility\ndef validate_order(order):\n    # validation logic\n    pass\n\ndef calculate_total(order):\n    # total calculation logic\n    pass\n\ndef save_order_to_database(order):\n    # save logic\n    pass\n\ndef process_order(order):\n    validate_order(order)\n    calculate_total(order)\n    save_order_to_database(order)\n\n\nAvoid Deep Nesting\nDeeply nested code can be hard to follow. Use early returns to reduce nesting levels.\nExample:\n# Bad: Deep nesting\ndef process_data(data):\n    if data is not None:\n        if len(data) &gt; 0:\n            if isinstance(data, list):\n                # process the list\n                pass\n\n# Good: Reduced nesting\ndef process_data(data):\n    if data is None or len(data) == 0 or not isinstance(data, list):\n        return\n\n    # process the list\n    pass",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html#clear-and-consistent-formatting",
    "href": "coding_practices/clean_code.html#clear-and-consistent-formatting",
    "title": "Code cleanliness",
    "section": "Clear and Consistent Formatting",
    "text": "Clear and Consistent Formatting\n\nFollow PEP 8\nAdhere to PEP 8, the Python style guide, to ensure consistent formatting. Key guidelines include:\n\nIndentation: Use 4 spaces per indentation level.\nLine Length: Limit all lines to a maximum of 79 characters.\nBlank Lines: Use blank lines to separate functions, classes, and sections within functions.\n\nTo make things easy, we will use ruff as a standard in our lab.\nExample:\n# PEP 8 compliant code\ndef calculate_area(radius):\n    import math\n    return math.pi * (radius ** 2)\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return calculate_area(self.radius)\n\n\nUse Docstrings\nUse docstrings to document your modules, classes, and functions. Docstrings should describe the purpose, parameters, and return values.\nExample:\ndef calculate_area(radius):\n    \"\"\"\n    Calculate the area of a circle given its radius.\n\n    Args:\n        radius (float): The radius of the circle.\n\n    Returns:\n        float: The area of the circle.\n    \"\"\"\n    import math\n    return math.pi * (radius ** 2)",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html#error-handling",
    "href": "coding_practices/clean_code.html#error-handling",
    "title": "Code cleanliness",
    "section": "Error Handling",
    "text": "Error Handling\n\nUse Exceptions Appropriately\nHandle exceptions gracefully and use specific exception types to avoid catching unexpected errors.\nExample:\n# Bad: Catching all exceptions\ntry:\n    result = 10 / 0\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Good: Catching specific exceptions\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError as e:\n    print(f\"Cannot divide by zero: {e}\")\n\n\nAvoid Empty except Clauses\nEmpty except clauses can hide errors and make debugging difficult. Always handle exceptions with proper logging or user feedback.\nExample:\n# Bad: Empty except clause\ntry:\n    # risky operation\n    pass\nexcept:\n    pass\n\n# Good: Proper exception handling\ntry:\n    # risky operation\n    pass\nexcept ValueError as e:\n    print(f\"ValueError occurred: {e}\")",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/clean_code.html#refactoring",
    "href": "coding_practices/clean_code.html#refactoring",
    "title": "Code cleanliness",
    "section": "Refactoring",
    "text": "Refactoring\nRegularly review and refactor code to improve structure and readability. Refactoring can include renaming variables, breaking down functions, or reorganizing code.\nKeeping your code tidy ensures it is readable and maintainable. Clean code is easier to understand, debug, and enhance. Here are some tips for maintaining code cleanliness:\nFor more detailed style guide, we will use Google Python Style Guide.",
    "crumbs": [
      "Coding practices",
      "Code cleanliness"
    ]
  },
  {
    "objectID": "coding_practices/code_decoupling.html#understanding-decoupling-in-code",
    "href": "coding_practices/code_decoupling.html#understanding-decoupling-in-code",
    "title": "Decoupled code",
    "section": "",
    "text": "Multiple Researchers: Different team members might work on different aspects of the codebase.\nEvolving Requirements: Research projects often evolve, requiring frequent changes and updates to the code.\nIntegration of Tools: Code might need to interface with various tools, libraries, and datasets.\n\n\n\nModularity: Breaking down the code into distinct modules or components.\nTestability: Isolated components are easier to test independently.\nMaintainability: Changes in one module have less chance of breaking other parts of the code.\nReusability: Modular components can be reused across different projects or experiments.",
    "crumbs": [
      "Coding practices",
      "Decoupled code"
    ]
  },
  {
    "objectID": "coding_practices/code_decoupling.html#principles-of-decoupled-python-code",
    "href": "coding_practices/code_decoupling.html#principles-of-decoupled-python-code",
    "title": "Decoupled code",
    "section": "Principles of Decoupled Python Code",
    "text": "Principles of Decoupled Python Code\n\nSingle Responsibility Principle (SRP)\nEach module or class should have only one reason to change, meaning it should have one primary responsibility. For instance, a module handling data cleaning should not be involved in data visualization.\nExample: Suppose you have a module named data_processing.py. It should focus solely on data cleaning and preprocessing, while another module, data_visualization.py, should handle plotting and generating graphs.\n# data_processing.py\ndef clean_data(data):\n    # Implementation of data cleaning\n    pass\n\ndef preprocess_data(data):\n    # Implementation of data preprocessing\n    pass\n\n# data_visualization.py\nimport matplotlib.pyplot as plt\n\ndef plot_data(data):\n    plt.plot(data)\n    plt.show()\n\n\nDependency Injection\nAvoid hard-coding dependencies within your functions or classes. Instead, pass dependencies as arguments. This approach makes your code more flexible and easier to test.\nExample: Instead of hardcoding a data source, pass it as a parameter.\n# Instead of this\ndef analyze_data():\n    data = load_data_from_file('data.csv')\n    # Process data\n\n# Use dependency injection\ndef analyze_data(data_loader):\n    data = data_loader()\n    # Process data\n\n# Example data loader function\ndef load_data_from_file():\n    return read_csv('data.csv')\n\n\nUse Interfaces and Abstract Classes\nDefine abstract classes or interfaces to specify the methods that concrete implementations should provide. This approach helps in decoupling the code from specific implementations and allows easier replacement or modification of components.\nExample:\nfrom abc import ABC, abstractmethod\n\nclass DataLoader(ABC):\n    @abstractmethod\n    def load(self):\n        pass\n\nclass CSVDataLoader(DataLoader):\n    def load(self):\n        # Load data from CSV\n        pass\n\nclass JSONDataLoader(DataLoader):\n    def load(self):\n        # Load data from JSON\n        pass\n\n\nSeparation of Concerns\nEnsure that different aspects of your code are managed separately. For example, keep data handling, computation, and presentation concerns distinct from each other.\nExample:\n# data_manager.py\ndef load_data(filename):\n    # Code to load data\n    pass\n\n# analysis_engine.py\ndef perform_analysis(data):\n    # Code to analyze data\n    pass\n\n# report_generator.py\ndef generate_report(results):\n    # Code to generate report\n    pass",
    "crumbs": [
      "Coding practices",
      "Decoupled code"
    ]
  },
  {
    "objectID": "coding_practices/code_decoupling.html#conclusion",
    "href": "coding_practices/code_decoupling.html#conclusion",
    "title": "Decoupled code",
    "section": "Conclusion",
    "text": "Conclusion\nDecoupled Python code is essential for maintaining clarity, flexibility, and reliability in a research lab setting. By adhering to principles such as the Single Responsibility Principle, dependency injection, and separation of concerns, researchers can create codebases that are easier to understand, test, and modify. Implementing these practices ensures that research software remains adaptable and robust, supporting the dynamic needs of scientific inquiry.\nFor more information on writing decoupled code, explore here.",
    "crumbs": [
      "Coding practices",
      "Decoupled code"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#code-comments",
    "href": "coding_practices/code_documentation.html#code-comments",
    "title": "Code documentation",
    "section": "Code Comments",
    "text": "Code Comments\nComments within the code explain specific lines or sections of code, making it easier to understand the logic and intentions of the code.\n\nInline Comments: Provide explanations for individual lines of code.\nBlock Comments: Explain larger sections or functions.\n\nExample:\ndef calculate_statistics(data):\n    \"\"\"\n    Calculate and return basic statistics for the given data.\n\n    Args:\n        data (list): A list of numerical values.\n\n    Returns:\n        tuple: A tuple containing the mean and standard deviation of the data.\n    \"\"\"\n    mean = sum(data) / len(data)  # Calculate mean\n    variance = sum((x - mean) ** 2 for x in data) / len(data)  # Calculate variance\n    std_dev = variance ** 0.5  # Calculate standard deviation\n    return mean, std_dev",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#docstrings",
    "href": "coding_practices/code_documentation.html#docstrings",
    "title": "Code documentation",
    "section": "Docstrings",
    "text": "Docstrings\nDocstrings are used to describe the purpose, parameters, and return values of functions, classes, and modules. They are essential for generating documentation automatically and integrating with IDEs and documentation tools.\n\nFunction Docstrings: Explain the purpose, arguments, and return values of a function.\nClass Docstrings: Describe the class’s purpose, attributes, and methods.\nModule Docstrings: Provide an overview of the module and its contents.\n\nExample:\nclass DataAnalyzer:\n    \"\"\"\n    A class for analyzing and processing data.\n\n    Attributes:\n        data (list): The data to be analyzed.\n\n    Methods:\n        calculate_mean(): Returns the mean of the data.\n        calculate_median(): Returns the median of the data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the DataAnalyzer with data.\n\n        Args:\n            data (list): The data to be analyzed.\n        \"\"\"\n        self.data = data\n\n    def calculate_mean(self):\n        \"\"\"\n        Calculate and return the mean of the data.\n\n        Returns:\n            float: The mean of the data.\n        \"\"\"\n        return sum(self.data) / len(self.data)",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#readme-files",
    "href": "coding_practices/code_documentation.html#readme-files",
    "title": "Code documentation",
    "section": "README Files",
    "text": "README Files\nREADME files provide an overview of the project, including its purpose, installation instructions, usage guidelines, and contribution details. They are often the first point of contact for users and contributors.\nExample:\n# Data Analysis Toolkit\n\n# Overview\nThis toolkit provides various utilities for analyzing and processing data.\n\n# Installation\nTo install the toolkit, run:\n```bash\npip install data-analysis-toolkit",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#sphinx",
    "href": "coding_practices/code_documentation.html#sphinx",
    "title": "Code documentation",
    "section": "Sphinx",
    "text": "Sphinx\nSphinx is a documentation generator that creates HTML, PDF, and other formats from reStructuredText files. It is often used for creating comprehensive project documentation.\n\nInstallation:\n\npip install sphinx\n\nUsage:\n\nCreate a Sphinx documentation directory:\nsphinx-quickstart\nGenerate documentation:\nsphinx-build -b html source build",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#write-clear-and-concise-descriptions",
    "href": "coding_practices/code_documentation.html#write-clear-and-concise-descriptions",
    "title": "Code documentation",
    "section": "Write Clear and Concise Descriptions",
    "text": "Write Clear and Concise Descriptions\nEnsure that descriptions are clear and avoid jargon. Provide enough detail to understand the purpose and functionality of the code without being overly verbose.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#update-documentation-regularly",
    "href": "coding_practices/code_documentation.html#update-documentation-regularly",
    "title": "Code documentation",
    "section": "Update Documentation Regularly",
    "text": "Update Documentation Regularly\nKeep documentation up-to-date with code changes. Outdated documentation can lead to confusion and errors.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#use-consistent-style",
    "href": "coding_practices/code_documentation.html#use-consistent-style",
    "title": "Code documentation",
    "section": "Use Consistent Style",
    "text": "Use Consistent Style\nFollow consistent formatting and style guidelines throughout the documentation. This includes using the same terminology, formatting conventions, and level of detail.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#include-examples",
    "href": "coding_practices/code_documentation.html#include-examples",
    "title": "Code documentation",
    "section": "Include Examples",
    "text": "Include Examples\nProvide examples to demonstrate how to use functions, classes, or modules. Examples help users understand practical applications of the code.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/code_documentation.html#document-edge-cases-and-limitations",
    "href": "coding_practices/code_documentation.html#document-edge-cases-and-limitations",
    "title": "Code documentation",
    "section": "Document Edge Cases and Limitations",
    "text": "Document Edge Cases and Limitations\nHighlight any edge cases, limitations, or known issues in the documentation. This prepares users for potential problems and guides them in handling exceptions.",
    "crumbs": [
      "Coding practices",
      "Code documentation"
    ]
  },
  {
    "objectID": "coding_practices/general_practices.html#chapter-code-development-workflow-for-research-labs",
    "href": "coding_practices/general_practices.html#chapter-code-development-workflow-for-research-labs",
    "title": "Good coding practices",
    "section": "",
    "text": "1. Introduction\nEffective code management is vital in research, especially for data analysis and simulations. This chapter outlines a simple workflow for developing Python code, using Conda to manage your environment and Ruff for code formatting.\n\n\n2. Setting Up Your Development Environment\n\nInstall Conda:\n\nWhat to Do: Download and install Miniconda or Anaconda from their websites. Miniconda is a lighter version with fewer pre-installed tools, while Anaconda comes with many tools already set up.\nWhy: Conda helps you create and manage isolated coding environments, ensuring that each project has the specific tools and libraries it needs.\n\nCreate and Manage Your Environment:\n\nCreate a New Environment: Open Conda and set up a new environment for your project. Choose a name for your environment and select the Python version you want to use.\nActivate the Environment: Switch to the new environment so you can install and work with the packages specific to your project.\nInstall Packages: Within this environment, install the necessary Python libraries for your project, such as NumPy, Pandas, and Scikit-Learn.\nInstall Ruff: Add Ruff to your environment for code formatting. This tool helps ensure your code follows consistent style rules and is easy to read.\nSave and Recreate Your Environment: Save the configuration of your environment so it can be recreated later or shared with others. You can use this saved configuration to set up the same environment on another machine.\n\n\n\n\n3. Writing and Organizing Code\n\nOrganize Your Project:\n\nFolders: Create a folder structure for your project. Use data/ for storing datasets, notebooks/ for Jupyter Notebooks, src/ for your main code files, tests/ for code tests, and scripts/ for commonly used tools or scripts.\nWhy: Organizing your project this way helps keep your work tidy and makes it easier to locate files and understand the project’s structure.\n\nVersion Control and Documentation:\n\nVersion Control: Use Git to track changes in your code. Set up a repository on GitHub or a similar platform to manage versions of your code and collaborate with others.\nDocumentation: Add comments to your code to explain what it does. Maintain a README.md file to provide an overview of your project, including how to install and use it.\n\nCode Formatting:\n\nUse Ruff: Format your code with Ruff to ensure it adheres to style guidelines. This helps make your code cleaner and more consistent. Run Ruff to check and automatically fix style issues in your code.\n\nTesting Your Code:\n\nWrite Tests: Develop tests for your code to verify that it works as expected. Use testing tools to create and run these tests. Store the test files in the tests/ folder.\n\n\n\n\n4. Managing Data\n\nOrganize and Track Data:\n\nStore Data: Keep raw data in data/raw/ and processed data in data/processed/.\nTrack Changes: Use data management tools to keep track of different versions of your data, which helps in maintaining consistency and reproducibility.\n\nAccess and Share Data:\n\nControl Access: Set up permissions to control who can view or modify your data.\nShare Data: Use platforms like Zenodo to share your datasets with others, ensuring they are properly cited.\n\n\n\n\n5. Ensuring Reproducibility\n\nRecreate Your Environment:\n\nWhat to Do: Use the saved environment configuration to recreate the same coding environment on different machines. This ensures that others can work with the same setup as you.\n\nReproducible Code:\n\nEnsure Execution: Make sure your code can be run by others without issues. Keep Jupyter Notebooks updated to help others replicate your results easily.\n\n\n\n\n6. Collaborating with Others\n\nCode Reviews and Communication:\n\nReview Code: Regularly review each other’s code and use tools like pull requests to integrate changes. This helps maintain code quality and facilitates knowledge sharing.\nCommunicate: Hold regular meetings to discuss progress, share updates, and document your findings collaboratively.\n\n\n\n\n7. Maintaining Your Project\n\nUpdate and Track Issues:\n\nKeep Updated: Regularly update your tools and libraries to ensure everything is up-to-date.\nManage Issues: Use issue tracking tools to handle bugs and feature requests efficiently.\n\n\n\n\n8. Conclusion\nBy following these steps, you’ll ensure that your coding practices are well-organized, reproducible, and collaborative. Using Conda for managing your environment and Ruff for formatting will enhance the efficiency and reliability of your work in the research lab.",
    "crumbs": [
      "Coding practices",
      "Good coding practices"
    ]
  },
  {
    "objectID": "python_resources.html#research-lab-guide-pytorch-libraries-and-tools",
    "href": "python_resources.html#research-lab-guide-pytorch-libraries-and-tools",
    "title": "Python resources",
    "section": "Research Lab Guide: PyTorch Libraries and Tools",
    "text": "Research Lab Guide: PyTorch Libraries and Tools\n\nIntroduction\nWelcome to our research lab!\n\n\n\n1. PyTorch Ignite \n\nGitHub Link: PyTorch Ignite\n\nPyTorch Ignite simplifies the development of neural network models by providing a high-level framework for training and evaluation. It abstracts away boilerplate code and facilitates the implementation of complex training loops, metrics computation, and model management. This allows researchers to focus more on experimenting with various architectures and algorithms.\n\n\n2. PyTorch Lightning \n\nGitHub Link: PyTorch Lightning\n\nPyTorch Lightning is our preferred framework for developing deep learning models in the lab. It automates the tedious aspects of deep learning research, such as setting up training loops, logging metrics, and handling distributed training across multiple GPUs or TPUs. By standardizing best practices and providing a clean interface, PyTorch Lightning enables rapid prototyping and seamless scaling of research projects.\nKey Features: - Automated Training Loop: Write minimal code to define your model, optimizer, and data loaders, and PyTorch Lightning takes care of the rest. - Reproducibility: Ensures deterministic training behavior for reproducible research results. - Scalability: Easily scale your models from single GPU to multi-GPU or even distributed training without changing your codebase. - Experiment Management: Built-in support for logging, visualization, and tracking experiments, making it easier to monitor and compare model performance.\nIn our lab, we predominantly use PyTorch Lightning for its ability to accelerate the development cycle and ensure consistency across experiments. It encourages best practices in deep learning research and facilitates collaboration among team members.\n\n\n3. RLPyT \n\nGitHub Link: RLPyT\n\nRLPyT specializes in reinforcement learning (RL) algorithms and frameworks within PyTorch. It offers modular implementations of state-of-the-art RL algorithms, parallelized environments for efficient training, and tools for benchmarking and performance evaluation.\n\n\n4. TorchFunc \n\nGitHub Link: TorchFunc\n\nTorchFunc provides utility functions and scripts to simplify common deep learning tasks, including data handling, model evaluation, and visualization. It enhances productivity by streamlining workflows and facilitating experimentation.\n\n\n5. PyTorch for NumPy Users \n\nGitHub Link: PyTorch for NumPy Users\n\nFor those transitioning from NumPy to PyTorch, this guide offers a comprehensive overview of PyTorch’s tensor operations, GPU utilization, and best practices. It ensures a smooth transition and empowers researchers to leverage PyTorch’s capabilities effectively.\n\n\n6. TorchProf \n\nGitHub Link: TorchProf\n\nTorchProf is a lightweight PyTorch profiler that helps identify performance bottlenecks within your models. It provides insights into memory usage and compute time for each layer, enabling optimizations for faster and more efficient model training.\n\n\n7. TorchTest \n\nGitHub Link: TorchTest\n\nTorchTest offers utilities for unit testing deep learning models in PyTorch. It ensures code reliability and consistency by validating model outputs, gradients, and expected behaviors across different experiments.\n\n\n\nConclusion\nThese PyTorch libraries and tools are indispensable for advancing your deep learning research in our lab. Whether you’re developing new models, optimizing performance, or ensuring code reliability, these resources will streamline your workflow and empower you to achieve impactful results.\nExplore the GitHub links provided for each tool to delve deeper into their functionalities and integration into your research projects. We encourage you to experiment, collaborate, and leverage these tools to push the boundaries of deep learning research.\nWelcome to the team, and we look forward to seeing your contributions using PyTorch Lightning and other powerful tools in our research endeavors!",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#a-journey-through-python-essentials",
    "href": "python_resources.html#a-journey-through-python-essentials",
    "title": "Python resources",
    "section": "",
    "text": "Python Tutor: Illuminating Your Path\nBegin your journey with Python Tutor, a sophisticated platform where code transcends the screen. Visualize your code execution step-by-step, gaining invaluable insights into Python’s mechanics. This interactive tool transforms abstract concepts into tangible understanding, making complex programming principles accessible and engaging.\nExplore Python Tutor here: Python Tutor\nGuide: - Utilize Python Tutor to visualize code execution and understand variable interactions dynamically. - Experiment with different code snippets to solidify your grasp on fundamental Python concepts.\n\n\nConda: Crafting Your Coding Sanctuaries\nDive deeper into efficient project management with Conda, a versatile tool for creating and managing Python environments. Whether you’re developing software or conducting research, Conda ensures seamless environment setup, eliminating dependency conflicts and promoting project reproducibility. Navigate Conda confidently to streamline your coding workflow and enhance collaboration.\nDiscover Conda’s capabilities here: Why You Need Python Environments and How to Manage Them with Conda\nGuide: - Master the installation and utilization of Conda to create isolated Python environments tailored to your project requirements. - Learn best practices for environment management to optimize development efficiency and maintain code integrity.\n\n\nOpen Source: Joining Forces in the Code World\nEngage with the dynamic world of open-source development through the Open Source Guide. Embrace collaboration and community-driven innovation as you navigate the principles of contributing effectively to open-source projects. From mastering version control with Git to exploring diverse projects aligned with your interests, empower yourself to make meaningful contributions and foster professional growth.\nJoin the open-source community here: Open Source Guide\nGuide: - Familiarize yourself with the ethos and methodologies of open-source development outlined in the Open Source Guide. - Enhance your proficiency in Git version control to efficiently manage code revisions and facilitate collaborative coding efforts.\n\n\nPyVideo: Learning from Experts\nImmerse yourself in PyVideo’s repository of educational content, where industry experts share profound insights and innovative practices in Python programming. From foundational concepts to advanced techniques and emerging trends, PyVideo enriches your learning journey with comprehensive tutorials and conference talks. Stay informed, inspired, and ahead in the ever-evolving Python ecosystem.\nExplore PyVideo’s resources here: PyVideo\nGuide: - Navigate PyVideo to access a wealth of video content covering diverse topics in Python programming. - Take notes and integrate key learnings from expert presentations into your coding practices and project development.\n\n\nWTF Python: Unveiling Python’s Intricacies\nDelve into the intriguing intricacies of Python with WTF Python—a curated collection of code snippets that unravel surprising behaviors and lesser-known features. Challenge yourself to decode these puzzling aspects of Python programming, gaining deeper insights into its flexibility and functionality. Discover the nuances that make Python a preferred language for diverse applications.\nUnveil Python’s mysteries here: WTF Python\nGuide: - Engage with WTF Python to explore unconventional Python behaviors and expand your programming horizons. - Experiment with provided code snippets to deepen your understanding of Python’s unique capabilities and edge cases.\n\n\nScientific Computing: Elevating Precision and Reproducibility\nElevate your scientific computing endeavors with “Good Enough Practices in Scientific Computing.” This comprehensive guide equips you with essential methodologies for optimizing code performance, ensuring data integrity, and promoting reproducibility in research settings. Adopt best practices to enhance the efficiency and transparency of your scientific coding workflows.\nEnhance your scientific practices here: Good Enough Practices in Scientific Computing\nGuide: - Navigate the guide to implement foundational practices in scientific programming, including effective data management and workflow optimization. - Incorporate version control strategies and virtual environments to maintain project integrity and facilitate collaboration in research environments.\n\n\nVisualizing NumPy: Mastering Data Manipulation\nCapitalize on NumPy’s prowess in numerical computing and data manipulation through interactive tutorials offered by Visualizing NumPy Operations. Enhance your proficiency in array operations, indexing techniques, and vectorized computations with visual demonstrations that illustrate complex concepts effectively. Strengthen your skills to tackle sophisticated data-centric challenges with confidence and precision.\nMaster NumPy’s capabilities here: Visualizing NumPy Operations\nGuide: - Explore interactive tutorials on Visualizing NumPy Operations to deepen your understanding of NumPy’s capabilities and applications. - Apply learned concepts to practical scenarios, refining your data manipulation skills and optimizing performance in computational tasks.\nEmbark on this enriching journey through Python essentials with enthusiasm and dedication. Each resource presented here serves as a gateway to expanding your programming prowess and mastering essential tools for real-world applications. Embrace curiosity, leverage these tools effectively, and let your coding journey unfold with innovation and excellence. Enjoy the adventure ahead—it’s a world of discovery waiting for you! 🚀",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#pycon",
    "href": "python_resources.html#pycon",
    "title": "Python resources",
    "section": "PyCon",
    "text": "PyCon\nPyCon is one of the best way to get in touch with great Python community. Lot of great talks and tutorials (highly recommended!).",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#ray",
    "href": "python_resources.html#ray",
    "title": "Python resources",
    "section": "Ray",
    "text": "Ray\nAnother python package we extensively use in our lab is Ray. Ray a powerful parallel processing library designed to streamline distributed computing tasks in Python. Whether you’re working with machine learning models, simulations, or data processing pipelines, Ray provides efficient tools for scaling your applications across multiple cores or clusters.\n\nKey Features of Ray\n\nTask Parallelism with Remote Functions\nRay enables you to define functions (@ray.remote) that can execute asynchronously across multiple workers. These functions can leverage the computing resources efficiently, executing tasks concurrently without blocking the main thread.\n\n\nActor Model for Stateful Services\nRay’s Actor model allows you to create and manage stateful objects that maintain their internal state across multiple function invocations. This is useful for building complex applications where maintaining state across distributed components is crucial.\n\n\nDistributed Data Processing with Ray Libraries\nRay comes with additional libraries that extend its capabilities for distributed computing:\n\nRay Tune: Scalable hyperparameter tuning. Ray Tune Documentation\nRay RLlib: Scalable reinforcement learning. Ray RLlib Documentation\nRay Serve: Scalable and efficient serving of machine learning models. Ray Serve Documentation\n\n\n\n\nResources and Documentation\nExplore more about Ray’s capabilities, including installation guides, tutorials, and API references, through its official documentation:\n\nRay GitHub Repository\nRay Documentation",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#tutorial-on-ray-a-parallel-processing-library",
    "href": "python_resources.html#tutorial-on-ray-a-parallel-processing-library",
    "title": "Python resources",
    "section": "Tutorial on Ray: A Parallel Processing Library",
    "text": "Tutorial on Ray: A Parallel Processing Library\nWelcome to the tutorial on Ray, a powerful parallel processing library designed to streamline distributed computing tasks in Python. Whether you’re working with machine learning models, simulations, or data processing pipelines, Ray provides efficient tools for scaling your applications across multiple cores or clusters. Let’s dive into the essentials of Ray and explore how you can leverage its capabilities effectively.\n\nKey Features of Ray\n\nTask Parallelism with Remote Functions\nRay enables you to define functions (@ray.remote) that can execute asynchronously across multiple workers. These functions can leverage the computing resources efficiently, executing tasks concurrently without blocking the main thread.\n\n\nActor Model for Stateful Services\nRay’s Actor model allows you to create and manage stateful objects that maintain their internal state across multiple function invocations. This is useful for building complex applications where maintaining state across distributed components is crucial.\n\n\nDistributed Data Processing with Ray Libraries\nRay comes with additional libraries that extend its capabilities for distributed computing:\n\nRay Tune: Scalable hyperparameter tuning. Ray Tune Documentation\nRay RLlib: Scalable reinforcement learning. Ray RLlib Documentation\nRay Serve: Scalable and efficient serving of machine learning models. Ray Serve Documentation\n\n\n\n\nResources and Documentation\nExplore more about Ray’s capabilities, including installation guides, tutorials, and API references, through its official documentation:\n\nRay GitHub Repository\nRay Documentation",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "python_resources.html#general-resources",
    "href": "python_resources.html#general-resources",
    "title": "Python resources",
    "section": "",
    "text": "Python Tutor:\nBegin your journey with Python Tutor, a sophisticated platform where code transcends the screen. Visualize your code execution step-by-step, gaining invaluable insights into Python’s mechanics. This interactive tool transforms abstract concepts into tangible understanding, making complex programming principles accessible and engaging.\nExplore Python Tutor here: Python Tutor\nGuide:\n\nUtilize Python Tutor to visualize code execution and understand variable interactions dynamically.\nExperiment with different code snippets to solidify your grasp on fundamental Python concepts.\n\n\n\nConda:\nDive deeper into efficient project management with Conda, a versatile tool for creating and managing Python environments. Whether you’re developing software or conducting research, Conda ensures seamless environment setup, eliminating dependency conflicts and promoting project reproducibility. Navigate Conda confidently to streamline your coding workflow and enhance collaboration.\nDiscover Conda’s capabilities: Why You Need Python Environments and How to Manage Them with Conda\nGuide:\n\nMaster the installation and utilization of Conda to create isolated Python environments tailored to your project requirements.\nLearn best practices for environment management to optimize development efficiency and maintain code integrity.\n\n\n\nOpen Source:\nEngage with the dynamic world of open-source development through the Open Source Guide. Embrace collaboration and community-driven innovation as you navigate the principles of contributing effectively to open-source projects. From mastering version control with Git to exploring diverse projects aligned with your interests, empower yourself to make meaningful contributions and foster professional growth.\nJoin the open-source community here: Open Source Guide\nGuide:\n\nFamiliarize yourself with the ethos and methodologies of open-source development outlined in the Open Source Guide.\nEnhance your proficiency in Git version control to efficiently manage code revisions and facilitate collaborative coding efforts.\n\n\n\nPyVideo:\nImmerse yourself in PyVideo’s repository of educational content, where industry experts share profound insights and innovative practices in Python programming. From foundational concepts to advanced techniques and emerging trends, PyVideo enriches your learning journey with comprehensive tutorials and conference talks. Stay informed, inspired, and ahead in the ever-evolving Python ecosystem.\nExplore PyVideo’s resources here: PyVideo\nGuide:\n\nNavigate PyVideo to access a wealth of video content covering diverse topics in Python programming.\nTake notes and integrate key learnings from expert presentations into your coding practices and project development.\n\n\n\nWTF Python:\nDelve into the intriguing intricacies of Python with WTF Python—a curated collection of code snippets that unravel surprising behaviors and lesser-known features. Challenge yourself to decode these puzzling aspects of Python programming, gaining deeper insights into its flexibility and functionality. Discover the nuances that make Python a preferred language for diverse applications.\nUnveil Python’s mysteries here: WTF Python\nGuide:\n\nEngage with WTF Python to explore unconventional Python behaviors and expand your programming horizons.\nExperiment with provided code snippets to deepen your understanding of Python’s unique capabilities and edge cases.\n\n\n\nScientific Computing:\nElevate your scientific computing endeavors with “Good Enough Practices in Scientific Computing.” This comprehensive guide equips you with essential methodologies for optimizing code performance, ensuring data integrity, and promoting reproducibility in research settings. Adopt best practices to enhance the efficiency and transparency of your scientific coding workflows.\nEnhance your scientific practices here: Good Enough Practices in Scientific Computing\nGuide:\n\nNavigate the guide to implement foundational practices in scientific programming, including effective data management and workflow optimization.\nIncorporate version control strategies and virtual environments to maintain project integrity and facilitate collaboration in research environments.\n\n\n\nVisualizing NumPy: Mastering Data Manipulation\nCapitalize on NumPy’s prowess in numerical computing and data manipulation through interactive tutorials offered by Visualizing NumPy Operations. Enhance your proficiency in array operations, indexing techniques, and vectorized computations with visual demonstrations that illustrate complex concepts effectively. Strengthen your skills to tackle sophisticated data-centric challenges with confidence and precision.\nMaster NumPy’s capabilities here: Visualizing NumPy Operations\nGuide:\n\nExplore interactive tutorials on Visualizing NumPy Operations to deepen your understanding of NumPy’s capabilities and applications.\nApply learned concepts to practical scenarios, refining your data manipulation skills and optimizing performance in computational tasks.",
    "crumbs": [
      "Python resources"
    ]
  },
  {
    "objectID": "bci_resources.html#introduction-to-bci-resources",
    "href": "bci_resources.html#introduction-to-bci-resources",
    "title": "BCI resources",
    "section": "Introduction to BCI Resources",
    "text": "Introduction to BCI Resources\nWelcome to the iHuman Lab! This guide will introduce you to essential resources and tools for understanding Brain-Computer Interfaces (BCI) and conducting research in this field.\n\nVideos\n\nSwartz Center for Computational Neuroscience\n\nIntroduction to BCI and EEG\n\nStart with the initial videos (video-1 to video-7) to grasp the fundamentals of BCI and EEG.\nThese videos provide a solid foundation for understanding how brain activity can be used to control external devices.\nWatch the videos here\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThese introductory videos are highly recommended as they cover the basics you need to get started.\n\n\n\n\nMike X Cohen\n\nGeneral Neural Time-Series Analysis\n\nExplore advanced concepts in neural time-series analysis, focusing on methods to analyze and interpret brain signals over time.\nThe videos primarily use Matlab, but you can replicate the analysis in Python using the mne package.\nWatch the videos here\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdapting Matlab scripts to Python will not only deepen your understanding of neural data analysis but also enhance your proficiency in Python programming.\n\n\n\n\n\nGitHub Links\n\nNeuroTechX/awesome-bci\n\nThis repository is a curated collection of resources, tools, and projects related to BCI research.\nExplore various software tools, datasets, and community-driven projects that can aid your research.\nVisit the repository\n\n\n\n\nBooks\n\nBrain-Computer Interfacing: An Introduction by Rajesh P.N. Rao\n\nThis book provides a comprehensive introduction to BCI technology, covering its principles, applications, and future trends.\nAccess the book\n\nAnalyzing Neural Time Series Data: Theory and Practice by Mike X. Cohen\n\nDelve into detailed methodologies and practical techniques for analyzing neural time-series data.\nGain insights into advanced signal processing techniques applicable to BCI research.\nExplore the book\n\n\n\n\nLab Streaming Layer (LSL)\n\nOverview and Importance\n\nLSL is a crucial middleware platform used in our lab to synchronize and record data from various sensors.\nIt simplifies data integration across different hardware and software platforms, ensuring seamless connectivity and interoperability.\nLearn more about LSL\n\n\n\nFeel free to approach any member of our lab for further guidance or clarification on these resources. We are excited to have you join us and look forward to seeing the innovative contributions you will make at iHuman Lab!",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "bci_resources.html#introduction-to-mne-python",
    "href": "bci_resources.html#introduction-to-mne-python",
    "title": "BCI resources",
    "section": "Introduction to MNE Python",
    "text": "Introduction to MNE Python\nAnother important package, we use extensively in our lab processing the physiological data is MNE-Python. MNE-Python is a Python package specifically designed for the analysis of brain signals, collected using techniques like magnetoencephalography (MEG) and electroencephalography (EEG). It provides a robust set of tools for processing, analyzing, and visualizing neural data, making it a preferred choice for researchers and clinicians in the field of neuroscience.\nKey features of MNE-Python include:\n\nData Handling: Import and manipulate data from various formats such as .fif, .edf, and .bdf.\nPreprocessing: Remove noise, filter data, and interpolate bad channels.\nAnalysis: Perform time-domain and frequency-domain analyses, including time-frequency decomposition.\nVisualization: Plot data in 2D and 3D to visualize sensor and source-space information.\nSource Localization: Estimate the location of neural sources responsible for recorded signals.\nStatistics: Conduct statistical tests to analyze differences in brain activity.\n\nMNE-Python supports a wide range of functionalities tailored to both novice users and advanced researchers, facilitating comprehensive exploration and interpretation of neuroimaging data.\nFor more information and detailed documentation, visit MNE-Python website.\n\nLab Streaming Layer (LSL)\nLab streaming layer (LSL) is an open-source networked middleware ecosystem to stream, receive, synchronize, and record neural, physiological, and behavioral data streams acquired from diverse sensor hardware.\nIt reduces complexity and barriers to entry for researchers, sensor manufacturers, and users through a simple, interoperable, standardized API to connect data consumers to data producers while abstracting obstacles such as platform differences, stream discovery, synchronization and fault-tolerance.\nWe use LSL as a standard platform to interface multiple sensor and simulation softwares in our lab.\n\nOverview and Importance\n\nLSL is a crucial middleware platform used in our lab to synchronize and record data from various sensors.\nIt simplifies data integration across different hardware and software platforms, ensuring seamless connectivity and interoperability.\nLearn more about LSL",
    "crumbs": [
      "BCI resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#introduction",
    "href": "eeg_resources.html#introduction",
    "title": "EEG resources",
    "section": "Introduction",
    "text": "Introduction\nWelcome to our research lab! We specialize in Human-robot interaction, employing EEG (Electroencephalography) among other modalities such as EMG and haptics. This guide will help you familiarize yourself with EEG analysis and related resources.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#eeg-glossary-1",
    "href": "eeg_resources.html#eeg-glossary-1",
    "title": "EEG resources",
    "section": "EEG Glossary",
    "text": "EEG Glossary\nTo understand EEG terminology and report formats, refer to the comprehensive glossary by clinical electroencephalographers: EEG Glossary.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#setting-up-eeg-recordings-1",
    "href": "eeg_resources.html#setting-up-eeg-recordings-1",
    "title": "EEG resources",
    "section": "Setting up EEG Recordings",
    "text": "Setting up EEG Recordings\nWatch this video tutorial on setting up advanced brain monitoring electrodes for EEG recording:\n\nFeel free to drop by the lab for more information.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#eeg-analysis-steps-1",
    "href": "eeg_resources.html#eeg-analysis-steps-1",
    "title": "EEG resources",
    "section": "EEG Analysis Steps",
    "text": "EEG Analysis Steps\nFor those who are starting with EEG analysis, it can be overwhelming to go across a lot of steps. Here is almost a standard procedure for a lot of EEG analysis. Each block is a research area by itself. There are a lot of papers on each block. Do not get scared once you start the analysis of some EEG data, these steps will become second to your nature. Do read below given paper about how others are doing these steps. We use mne-python to implement different block functionalities. If you want more information, refer to Mike X Cohen’s lectures\n\n\n\neeg-analysis",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#mne-mne-python",
    "href": "eeg_resources.html#mne-mne-python",
    "title": "EEG resources",
    "section": "MNE (MNE-Python)",
    "text": "MNE (MNE-Python)\nWe extensively use mne-python for EEG data analysis. Resources to get you started:\n\nBasic mne-python tutorial\nIntermediate tutorials\nJoin the mailing list for updates and community support.",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#eeg-databases-1",
    "href": "eeg_resources.html#eeg-databases-1",
    "title": "EEG resources",
    "section": "EEG Databases",
    "text": "EEG Databases\nHere are some databases on which you are apply your mne-python skills.\n\n\n\n\n\n\nTip\n\n\n\nThe best way to learn a (Python) package is to use it on your own data and see what are the capabilities\n\n\nPick up some data from this repository: meagmohit/EEG-Datasets. Use mne-python do some analysis and that is how you get introduced to analysis of EEG.\n\n\n\n\n\n\nTip\n\n\n\nIf you have doubts, do drop by to iHuman lab and ask any questions!",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#key-papers",
    "href": "eeg_resources.html#key-papers",
    "title": "EEG resources",
    "section": "Key Papers",
    "text": "Key Papers\nExplore these essential papers on EEG data analysis:\n\nBest Practices in Data Analysis and Sharing in Neuroimaging using MEEG\nReproducible MEG/EEG Group Study With the MNE Software\nEEG artifact removal—state-of-the-art and guidelines",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "eeg_resources.html#github-links-1",
    "href": "eeg_resources.html#github-links-1",
    "title": "EEG resources",
    "section": "GitHub Links",
    "text": "GitHub Links\nExplore these repositories and resources for EEG and signal processing:\n\nAnalysis Related Repositories\n\npyprep\nautoreject\neegfaster\nneurodsp\nBioSPPy\nEMG-Signal-Processing-Library\nPython-for-Signal-Processing\n\n\n\nUseful Libraries and Links\n\nBest Python Libraries for Psychology\nNeuroDataDesign\nEEG Notebooks\nOpenTools\nSWIPE4ICA for EEG artifact identification\nLabeling Tool\n\n\n\nDeep Learning with Physiological Signals\n\nARL EEG Models\nDeepEEG\nBraindecode\nMNE-Torch\n\n\nThis guide aims to provide you with foundational resources and tools to excel in EEG research. Don’t hesitate to visit the iHuman lab for further assistance and hands-on training. Happy researching!",
    "crumbs": [
      "EEG resources"
    ]
  },
  {
    "objectID": "lab_resources.html",
    "href": "lab_resources.html",
    "title": "Lab resources",
    "section": "",
    "text": "Item\nUnits\nDescription\nDocumentation\n\n\n\n\nPanda Robot\n1\nAdvanced robotic platform for research and interaction.\nPanda Robot Docs\n\n\nLamda GPU Computers\n2\nHigh-performance computers equipped with GPUs.\nLamda GPU Computers Docs\n\n\nEEG Headset\n1\nDevice for capturing brain activity through EEG signals.\nEEG Headset Docs\n\n\nEEG Software\n1\nSoftware for processing and analyzing EEG data.\nEEG Software Docs\n\n\nEye Tracking\n2\nEquipment for tracking eye movements and gaze patterns.\nEye Tracking Docs\n\n\nEMG Device\n2\nDevice for measuring electrical activity in muscles.\nEMG Device Docs\n\n\nAR/VR Headset\n1\nHead-mounted display for augmented and virtual reality.\nAR/VR Headset Docs\n\n\nSmall Drones\n50\nLightweight unmanned aerial vehicles for various experiments.\nSmall Drones Docs\n\n\nDepth Sensors\n2\nSensors for measuring depth and distance in environments.\nDepth Sensors Docs\n\n\nLab Computers\n5\nStandard desktop computers for general lab use.\nLab Computers Docs\n\n\nLab Equipment\n5\nVarious equipment used in the laboratory for experiments.\nLab Equipment Docs\n\n\nOn-board Computers\n3\nMicro controllers for on-board processing in experiments.\nOn-board Computers Docs\n\n\nLaptops\n3\nPortable laptops for mobile computing needs.\nLaptops Docs",
    "crumbs": [
      "Lab resources"
    ]
  },
  {
    "objectID": "writing_practices/general_practices.html",
    "href": "writing_practices/general_practices.html",
    "title": "Writing principles",
    "section": "",
    "text": "Using GPT\nUsing GPT as a writing tool can be immensely beneficial, but it’s important to approach it with care and use it as a supportive resource rather than a crutch. Here’s how to leverage ChatGPT effectively while maintaining control over your writing process:",
    "crumbs": [
      "Writing principles"
    ]
  },
  {
    "objectID": "writing_practices/general_practices.html#using-chatgpt",
    "href": "writing_practices/general_practices.html#using-chatgpt",
    "title": "General practices",
    "section": "",
    "text": "Generate Ideas and Inspiration: ChatGPT can help spark ideas and provide inspiration for your writing projects. Use it to brainstorm topics, explore different angles, or overcome writer’s block by generating prompts or outlines.\nImprove Clarity and Structure: Ask ChatGPT for suggestions on structuring your writing. It can provide guidance on organizing your thoughts logically, creating smooth transitions between paragraphs, or refining the overall flow of your piece.\nEnhance Language and Style: Use ChatGPT to improve the clarity and coherence of your sentences. It can offer alternative word choices, help with phrasing, and suggest improvements in grammar and syntax.\nEditing and Proofreading: Use ChatGPT to review your draft for spelling errors, grammatical mistakes, and punctuation issues. It can serve as a preliminary editor, but final proofreading should always be done by you.\nFeedback and Revision Suggestions: Seek feedback from ChatGPT on the overall effectiveness of your writing. It can provide insights into areas that may need further development or clarification.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nMaintaining ownership of your ideas and writing style while using ChatGPT is crucial to preserving the authenticity and integrity of your work. As a tool, ChatGPT offers valuable assistance in refining and enhancing your writing, but it should not overshadow your unique voice as a writer. Your critical thinking skills play a pivotal role here; they allow you to evaluate the suggestions provided by ChatGPT thoughtfully and decide how best to integrate them into your writing.\nWhile it can generate text based on input and provide suggestions, not all responses may be equally relevant or of high quality. It’s essential to scrutinize the responses, considering factors such as accuracy, coherence, and appropriateness for your intended audience. Evaluate each suggestion critically to ensure it aligns with the tone, style, and purpose of your writing.\nBy leveraging ChatGPT mindfully as a writing tool, you can indeed enhance your writing skills and streamline your workflow. It can help you achieve clearer and more polished pieces by offering insights into structure, language refinement, and even creative ideation. However, it’s paramount to maintain a balanced approach. Avoid relying solely on ChatGPT for the creative and intellectual aspects of your writing process. Remember, your originality and ability to engage with your subject matter in a meaningful way are what ultimately define your work as uniquely yours.",
    "crumbs": [
      "Writing practices",
      "General practices"
    ]
  },
  {
    "objectID": "writing_practices/general_practices.html#using-gpt",
    "href": "writing_practices/general_practices.html#using-gpt",
    "title": "Writing principles",
    "section": "",
    "text": "Generate Ideas and Inspiration ChatGPT can help spark ideas and provide inspiration for your writing projects. Use it to brainstorm topics, explore different angles, or overcome writer’s block by generating prompts or outlines.\nImprove Clarity and Structure Ask ChatGPT for suggestions on structuring your writing. It can provide guidance on organizing your thoughts logically, creating smooth transitions between paragraphs, or refining the overall flow of your piece.\nEnhance Language and Style Use ChatGPT to improve the clarity and coherence of your sentences. It can offer alternative word choices, help with phrasing, and suggest improvements in grammar and syntax.\nEditing and Proofreading Use ChatGPT to review your draft for spelling errors, grammatical mistakes, and punctuation issues. It can serve as a preliminary editor, but final proofreading should always be done by you.\nFeedback and Revision Suggestions Seek feedback from ChatGPT on the overall effectiveness of your writing. It can provide insights into areas that may need further development or clarification.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nMaintaining ownership of your ideas and writing style while using GPT is crucial to preserving the authenticity and integrity of your work. As a tool, ChatGPT offers valuable assistance in refining and enhancing your writing, but it should not overshadow your unique voice as a writer. Your critical thinking skills play a pivotal role here; they allow you to evaluate the suggestions provided by ChatGPT thoughtfully and decide how best to integrate them into your writing.\nWhile it can generate text based on input and provide suggestions, not all responses may be equally relevant or of high quality. It’s essential to scrutinize the responses, considering factors such as accuracy, coherence, and appropriateness for your intended audience. Evaluate each suggestion critically to ensure it aligns with the tone, style, and purpose of your writing.\nBy leveraging GPT mindfully as a writing tool, you can indeed enhance your writing skills and streamline your workflow. It can help you achieve clearer and more polished pieces by offering insights into structure, language refinement, and even creative ideation. However, it’s paramount to maintain a balanced approach. Avoid relying solely on GPT for the creative and intellectual aspects of your writing process. Remember, your originality and ability to engage with your subject matter in a meaningful way are what ultimately define your work as uniquely yours.",
    "crumbs": [
      "Writing principles"
    ]
  },
  {
    "objectID": "writing_practices/making_diagrams.html",
    "href": "writing_practices/making_diagrams.html",
    "title": "Making diagrams",
    "section": "",
    "text": "Links\nThe true essence of tools emerges only when they are wielded by an artist—the researcher. A diagramming tool, on its own, is a set of digital functionalities and features. It is through the skilled hands of the scientist that these tools transcend mere utility to become vehicles of clarity and innovation.\nHere are some links/tutorials which help you hone your diagram making skills. Please note that these resources are not exhaustive.",
    "crumbs": [
      "Writing principles",
      "Making diagrams"
    ]
  },
  {
    "objectID": "writing_practices/making_diagrams.html#links",
    "href": "writing_practices/making_diagrams.html#links",
    "title": "Making diagrams",
    "section": "",
    "text": "General design tips\n\n\nInkscape tutorials",
    "crumbs": [
      "Writing principles",
      "Making diagrams"
    ]
  },
  {
    "objectID": "writing_practices/making_presentations.html",
    "href": "writing_practices/making_presentations.html",
    "title": "Making presentations",
    "section": "",
    "text": "General guidlines",
    "crumbs": [
      "Writing principles",
      "Making presentations"
    ]
  },
  {
    "objectID": "writing_practices/making_presentations.html#general-guidlines",
    "href": "writing_practices/making_presentations.html#general-guidlines",
    "title": "Making presentations",
    "section": "",
    "text": "Face the audience, speak to them!\nDon’t read the slides\nDon’t put to much writing\nDon’t use color maps that are not blind-color friendly\nDon’t write long equations, no one can follow those\nPlease respect your time, it’s the audience time and other speakers time too.\nKeep the number of slides under the number of total minutes\nKeep the number of messages per slide to one, or at most…one\nDon’t be surprised by the next slide (like, “crap now I have to explain this slide”)\nDon’t put on a full manuscript table and then say “I know you won’t be able to read this”\nAlso, don’t skip slides in front of your audience like no one is noticing. “Sorry, just want skip these here for the sake of time… Ok… Oh, one more. Yes. As I was saying…”\nIf you show an equation, give an intuition for what it’s telling you. The title of a data slide should (almost always) be a SINGLE SENTENCE that tells the audience what you concluded from the data",
    "crumbs": [
      "Writing principles",
      "Making presentations"
    ]
  },
  {
    "objectID": "writing_practices/making_presentations.html#preparing-the-slides",
    "href": "writing_practices/making_presentations.html#preparing-the-slides",
    "title": "Making presentations",
    "section": "Preparing the slides",
    "text": "Preparing the slides\n\nIf you don’t explain it, don’t put it on the slide.\nGive slides titles that state the conclusion of the data\nAdd animations!\nConvince the audience what you plan to tell them is worth the effort of listening VERY early in the talk. And one “idea” per slide (this doesn’t mean Fig1A-H copy pasted from your manuscript).\nMake sure figures (and their axes labels) are big enough (don’t make me squint)\nPhysically point to the diagram during explanations\nMore visuals than text!\nStatements to avoid:\n\n“I know there’s a lot of data here, but I’m only going to talk about some of it” - don’t put data there if you don’t talk about it\n“Um” - try to speak clearly\n“You may not be able to see this, but…” - make the figure bigger\n\nAnd if it’s for you to show things, don’t put any words. People CANNOT READ AND LISTEN at the same time. So many people can’t understand this simple fact.\nHave a conclusion slide that summarizes the key takeaways from the presentation. During Q&A, stay on this conclusion slide (allowing it to sink in to the audience) instead of a slide that simply says “Thank you”, or “Questions?” or something like that.\nDon’t overestimate the background knowledge of your audience. Spend a few extra minutes explaining the background of your research, why it’s interesting, and defining any special terms you use. As long as you’re not patronizing, everyone will appreciate it.\nDon’t say “bear with me” or “this looks complicated but…” or anything that implies that your audience is not good, smart, quick enough to get you…\nHave backup slides. You can anticipate or will experience common questions or follow-on detail/data that audiences will ask.\nDescribe both axes of a graph on every single slide. You know the data. Your audience doesn’t. Orient them.\nI TRY to maintain one idea per slide. New idea or piece of data? New slide.\nBullet points are for suckers. Use an image, and talk over it. This is my “spray and pray marketing” slide.",
    "crumbs": [
      "Writing principles",
      "Making presentations"
    ]
  },
  {
    "objectID": "writing_practices/making_presentations.html#links",
    "href": "writing_practices/making_presentations.html#links",
    "title": "Making presentations",
    "section": "Links",
    "text": "Links\n\n\n\n\n\n\nTask\n\n\n\nComplete the free Coursera course on making presentations: Presentation skills: Designing Presentation Slides",
    "crumbs": [
      "Writing principles",
      "Making presentations"
    ]
  },
  {
    "objectID": "writing_practices/making_posters.html",
    "href": "writing_practices/making_posters.html",
    "title": "Making posters",
    "section": "",
    "text": "How to prepare a ‘classic’ Scientific Poster\nWhile the status quo of scientific posters is far from perfect (see video above), academic supervisors, organizations and conference may require scientific posters in a pre-defined format. Below you will find a few tips to prepare a scientific poster that meets those expectations.",
    "crumbs": [
      "Writing principles",
      "Making posters"
    ]
  },
  {
    "objectID": "writing_practices/general_principles.html",
    "href": "writing_practices/general_principles.html",
    "title": "Writing principles",
    "section": "",
    "text": "Using GPT\nUsing GPT as a writing tool can be immensely beneficial, but it’s important to approach it with care and use it as a supportive resource rather than a crutch. Here’s how to leverage ChatGPT effectively while maintaining control over your writing process:",
    "crumbs": [
      "Writing principles"
    ]
  },
  {
    "objectID": "writing_practices/general_principles.html#using-gpt",
    "href": "writing_practices/general_principles.html#using-gpt",
    "title": "Writing principles",
    "section": "",
    "text": "Generate Ideas and Inspiration ChatGPT can help spark ideas and provide inspiration for your writing projects. Use it to brainstorm topics, explore different angles, or overcome writer’s block by generating prompts or outlines.\nImprove Clarity and Structure Ask ChatGPT for suggestions on structuring your writing. It can provide guidance on organizing your thoughts logically, creating smooth transitions between paragraphs, or refining the overall flow of your piece.\nEnhance Language and Style Use ChatGPT to improve the clarity and coherence of your sentences. It can offer alternative word choices, help with phrasing, and suggest improvements in grammar and syntax.\nEditing and Proofreading Use ChatGPT to review your draft for spelling errors, grammatical mistakes, and punctuation issues. It can serve as a preliminary editor, but final proofreading should always be done by you.\nFeedback and Revision Suggestions Seek feedback from ChatGPT on the overall effectiveness of your writing. It can provide insights into areas that may need further development or clarification.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nMaintaining ownership of your ideas and writing style while using GPT is crucial to preserving the authenticity and integrity of your work. As a tool, ChatGPT offers valuable assistance in refining and enhancing your writing, but it should not overshadow your unique voice as a writer. Your critical thinking skills play a pivotal role here; they allow you to evaluate the suggestions provided by ChatGPT thoughtfully and decide how best to integrate them into your writing.\nWhile it can generate text based on input and provide suggestions, not all responses may be equally relevant or of high quality. It’s essential to scrutinize the responses, considering factors such as accuracy, coherence, and appropriateness for your intended audience. Evaluate each suggestion critically to ensure it aligns with the tone, style, and purpose of your writing.\nBy leveraging GPT mindfully as a writing tool, you can indeed enhance your writing skills and streamline your workflow. It can help you achieve clearer and more polished pieces by offering insights into structure, language refinement, and even creative ideation. However, it’s paramount to maintain a balanced approach. Avoid relying solely on GPT for the creative and intellectual aspects of your writing process. Remember, your originality and ability to engage with your subject matter in a meaningful way are what ultimately define your work as uniquely yours.",
    "crumbs": [
      "Writing principles"
    ]
  },
  {
    "objectID": "writing_practices/writing_articles.html",
    "href": "writing_practices/writing_articles.html",
    "title": "Writing articles",
    "section": "",
    "text": "If you’re diving into research or getting started with writing scientific articles, LaTeX is your friend. LaTeX (pronounced “LAY-tech”) is not just any tool—it’s a powerful typesetting system that helps you create beautifully formatted documents, perfect for academic and scientific writing. In our lab, we almost exclusively use LaTeX for most of the documents.\n\nWhy Choose LaTeX?\n\nPolished Documents: It makes your research papers and lab manuals look super professional with neat equations, figures, and references.\nFocus on Content: You get to focus on what really matters—your research—without getting bogged down by formatting headaches.\nCollaboration Made Easy: LaTeX plays well with others. It’s perfect for working on documents with classmates or colleagues, thanks to its compatibility with tools like Overleaf.\n\n\n\n\n\n\n\nTask\n\n\n\n\nHead over to Overleaf and sign up—it’s free!\nClick on “New Project” and choose “Blank Project”. This sets you up with a clean slate to start writing your document.\nOverleaf shows your document in two parts: on the left, you write your LaTeX code (don’t worry, it’s easier than it sounds!), and on the right, you instantly see how it looks as a document.\n\n\n\nTo get started with LaTeX in Overleaf, follow the tutorials below\n\n\n\nFor more detailed tutorials, please see the Overleaf Documentation",
    "crumbs": [
      "Writing principles",
      "Writing articles"
    ]
  },
  {
    "objectID": "writing_practices/making_posters.html#how-to-prepare-a-classic-scientific-poster",
    "href": "writing_practices/making_posters.html#how-to-prepare-a-classic-scientific-poster",
    "title": "Making posters",
    "section": "",
    "text": "How to evaluate a Scientific Poster\n\nBasics\n\nTitle and other required sections are present\nComplete author affiliation & contact information is included\nThe poster conforms to the requirements of the conference or program where it will be presented\nFont is consistent throughout\nSpelling is correct throughout\nGrammar is correct throughout\nAcronyms are defined on first use\nContent is appropriate & relevant for audience\n\n\n\nDesign\n\nAll text can be easily read from 4 feet away\nFlow of the poster is easy to follow\nWhite space used well\nSection titles are used consistently\nImages/graphics are used in place of text whenever possible\nBullet points/lists are used in place of text whenever possible\nAll images are relevant and necessary to the poster\nCharts are correct – i.e. appropriate type for data, data is correct & correctly represented\nText color and background color are significant in contrast for easy reading\nBackground color doesn’t obscure or dim text\nImages are clear, not pixilated or blurry\n\n\n\nContent\n\nThe “story” of the poster is clear\nThe content is focused on 2-3 key points\nTitle is clear & informative of the project\nProblem, or clinical question, is identified and explained\nCurrent evidence related to project is listed\nObjectives are stated\nMethods are described\nResults are presented\nConclusions are stated\nImplications to practice and to other professions are presented\nReferences are listed\nAll content is relevant and on the key points\nContent is not duplicated in text and graphics\n\n\n\nOral Presentation\n\nPresenter greets people\nPresenter is able to give a concise synopsis of poster\nPresenter is able to explain all diagrams and sections\nPresenter speaks fluently – i.e. doesn’t stumble, leave sentences/thoughts hanging\nPresenter has questions to ask viewers\n\nMore tips:",
    "crumbs": [
      "Writing principles",
      "Making posters"
    ]
  }
]